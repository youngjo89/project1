{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pylab \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics import utils\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "weather = pd.read_csv('../data/weather.csv')\n",
    "key = pd.read_csv('../data/key.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample = pd.read_csv('../data/sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TM_transform(series, T_replace, M_replace):  # Temporary solution\n",
    "    \"\"\"\n",
    "    데이터내의 T, M을 원하는 값으로 바꿔주는 함수\n",
    "    TM_transform(series, T_replace)\n",
    "    \"\"\"\n",
    "    series = series.astype(str).map(lambda s: s.strip())\n",
    "    series[series == 'T'] = T_replace\n",
    "    series[series == 'M'] = M_replace\n",
    "    return series.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_item_nbr(df) : # 모든 units이 0이 아닌 item_nbr을 구하는 함수, list형태로 return\n",
    "    tmp = df.pivot_table(values = 'log_units', index = ['date'], columns = ['item_nbr'])\n",
    "    tmp = tmp.loc[:, (tmp != 0).any(axis = 0)]\n",
    "    tmp.loc['2012-12-25'] = 0 # 2012-12-25가 빠져있음 train data에서.. 그래서 log_units = 0으로 넣어줌.\n",
    "    \n",
    "    tmp.reset_index(inplace = True)\n",
    "    tmp.sort_values(by = 'date', inplace = True)\n",
    "    tmp.drop(['date'], axis = 1, inplace = True)\n",
    "    \n",
    "    result = list(tmp.columns)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_dateformat(df, year):\n",
    "    \"\"\"\n",
    "    영문 월을 숫자 월로 바꾸어주고 나중에 사용하기 쉽도록 datetime.date 형태로 바꾸어주는 함수\n",
    "    \"\"\"\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    for i in range(len(df)):\n",
    "        dates = df.loc[i][0]\n",
    "        dates = dates.split(\" \")\n",
    "        for j in range(len(months)):\n",
    "            if dates[0] == months[j]:\n",
    "                dates[0] = str(j + 1)\n",
    "                dates_df = [\"{} {} {}\".format(year, dates[0], dates[1])]\n",
    "                dates_df = pd.to_datetime(dates_df)\n",
    "                df.loc[i][0] = dates_df.date[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_holiday(holiday_df1, holiday_df2, holiday_df3):\n",
    "    \"\"\"\n",
    "    각 연도별 공휴일 리스트 합치기\n",
    "    \"\"\"\n",
    "    frame = [holiday_df1, holiday_df2, holiday_df3]\n",
    "    holiday = pd.concat(frame).reset_index(drop=True)\n",
    "    return holiday\n",
    "\n",
    "def find_holiday(file, year):\n",
    "    \"\"\"\n",
    "    수요에 영향을 미치는 주요 공휴일을 찾아내는 함수\n",
    "    \"\"\"\n",
    "    holidays = [\"New Year's Day\", \"Martin Luther King Jr. Day\", \"Valentine's Day\",  \"President's Day\", \"Easter Sunday\", \n",
    "                      \"Mother's Day\", \"Memorial Day\", \"Father's Day\", \"Independence Day\", \"Labor Day\", \"Columbus Day\",\n",
    "                      \"Halloween\", \"Veterans Day\", \"Thanksgiving Day\", \"Black Friday\", \"Christmas Eve\", \"Christmas Day\", \"New Year's Eve\"]\n",
    "    \n",
    "    holi = pd.read_excel(file, year, header=None)\n",
    "    holi = match_dateformat(holi, year)\n",
    "    holiday = pd.DataFrame(columns=[0,1,2,3,4])\n",
    "    for _ in holidays:\n",
    "        for i in range(len(holi[2])):\n",
    "            if _ == holi[2][i]:\n",
    "                holiday = holiday.append(holi.loc[i])\n",
    "    return holiday\n",
    "\n",
    "def cs_preprocessing(codesum):\n",
    "    codesum_temp = []\n",
    "    for _ in codesum:\n",
    "        _ = _.replace('+', '')\n",
    "        _ = _.replace('-', '')\n",
    "        if len(_) > 2:\n",
    "            _1 = _[:2]\n",
    "            codesum_temp.append(_1)\n",
    "            _2 = _[2:]\n",
    "            codesum_temp.append(_2)\n",
    "        else:\n",
    "            codesum_temp.append(_)\n",
    "    codesum = codesum_temp\n",
    "    return codesum\n",
    "\n",
    "def weather_flagger(weather):\n",
    "    codesum_ls = ['FC', 'TS', 'GR', 'RA', 'DZ', 'SN', 'SG', 'GS', 'PL', 'IC', 'FG', 'BR', 'UP', 'HZ', 'FU', 'VA', 'DU', 'DS', 'PO', 'SA', 'SS', 'PY', 'SQ', 'DR', 'SH', 'FZ', 'MI', 'PR', 'BC', 'BL', 'VC']\n",
    "    weather['date'] = pd.to_datetime(weather['date']) #weather는 글로벌변수\n",
    "    for i in range(len(weather['codesum'])):\n",
    "        codesum = weather['codesum'][i].split(\" \")\n",
    "        codesum = cs_preprocessing(codesum)\n",
    "        for _ in codesum:\n",
    "            flag = any(code in _ for code in codesum_ls)\n",
    "            if flag == True:\n",
    "                weather.set_value(i, '{}_flag'.format(_), 1)\n",
    "            else:\n",
    "                weather.set_value(i, 'normal_flag', 1)\n",
    "    weather['snowfall'] = TM_transform(weather['snowfall'], 0.02, 0.0)\n",
    "    weather['preciptotal'] = TM_transform(weather['preciptotal'], 0.02, 0.0)\n",
    "    weather['snow_event'] = np.where(np.where(weather['SN_flag'] == 1, 1, 0) + np.where(weather['snowfall'] > 2, 1, 0) == 2, 1, 0)\n",
    "    weather['rain_event'] = np.where(np.where(weather['RA_flag'] == 1, 1, 0) + np.where(weather['preciptotal'] > 1, 1, 0) == 2, 1, 0)\n",
    "    weather['event'] = weather['snow_event'] + weather['rain_event']\n",
    "    weather['event'] = np.where(weather['event'] >= 1, 1, 0)\n",
    "    return weather\n",
    "\n",
    "def preprocessing(df, holiday, weather):\n",
    "    \"\"\"\n",
    "    train데이터를 가공하는 함수\n",
    "    \"\"\"\n",
    "    df['log_units'] = np.log(df['units'] + 1) # logged units\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['weekday'] = df.date.dt.weekday  # 월요일이 0 일요일이 6\n",
    "    df['weekend'] = df.date.dt.weekday.isin([5, 6])  # 5: 토요일, 6: 일요일\n",
    "\n",
    "    df['holiday'] = df.date.isin(holiday[0])\n",
    "    df['weekday_holiday'] = df.holiday & (df.weekend == False)\n",
    "    df['weekend_holiday'] = df.holiday & df.weekend\n",
    "    \n",
    "    df = pd.merge(df, key, on='store_nbr') #key는 글로벌 변수\n",
    "    df = pd.merge(df, weather[['date', 'station_nbr', 'tmax', 'tmin', 'tavg', 'depart', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "                               'sunrise', 'sunset', 'codesum', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed',\n",
    "                               'resultspeed', 'resultdir', 'avgspeed' ,'event']], on=['date', 'station_nbr'])\n",
    "    return df\n",
    "\n",
    "def test_preprocessing(df, holiday, weather):\n",
    "    weather = weather_flagger(weather)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['weekday'] = df.date.dt.weekday  # 월요일이 0 일요일이 6\n",
    "    df['weekend'] = df.date.dt.weekday.isin([5, 6])  # 5: 토요일, 6: 일요일\n",
    "\n",
    "    df['holiday'] = df.date.isin(holiday[0])\n",
    "    df['weekday_holiday'] = df.holiday & (df.weekend == False)\n",
    "    df['weekend_holiday'] = df.holiday & df.weekend\n",
    "    \n",
    "    df = pd.merge(df, key, on='store_nbr') #key는 글로벌 변수\n",
    "    df = pd.merge(df, weather[['date', 'station_nbr', 'tmax', 'tmin', 'tavg', 'depart', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "                               'sunrise', 'sunset', 'codesum', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed',\n",
    "                               'resultspeed', 'resultdir', 'avgspeed' ,'event']], on=['date', 'station_nbr'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def by_store(df, nbr) :\n",
    "    df_new = df[df['store_nbr'] == nbr]\n",
    "    df_new.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train[train['date'] <= '2013-03-31'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_df = weather[weather['date'] <= '2013-03-31'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holiday12 = find_holiday('../data/holiday.xlsx', '2012')\n",
    "holiday13 = find_holiday('../data/holiday.xlsx', '2013')\n",
    "holiday14 = find_holiday('../data/holiday.xlsx', '2014')\n",
    "holiday = merge_holiday(holiday12, holiday13, holiday14)\n",
    "weather_df = weather_flagger(weather_df)\n",
    "processed_train = preprocessing(train, holiday, weather_df)\n",
    "processed_test = test_preprocessing(test, holiday, weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분포 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS의 기본 가정인 종속 변수 y가 독립변수 x의 선형 조합으로 결정되는 기댓값과 고정된 분산 $\\sigma^{2}$를 가지는 정규 분포인지 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_norm_test = train_df.pivot_table(values='units', index=['store_nbr', 'date'], columns=['item_nbr'])\n",
    "train_df['log_units'] = np.log(train_df['units'] + 1)\n",
    "train_norm_test_log = train_df.pivot_table(values='log_units', index=['store_nbr', 'date'], columns=['item_nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's draw per item_nbr per store_nbr * with units\n",
    "for i in range(1,2):  #원래는 1~45\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    t = []\n",
    "    temp = train_norm_test.loc[i]\n",
    "    temp2 = train_norm_test_log.loc[i]\n",
    "    temp = temp.loc[:, (temp !=0).any(axis=0)]\n",
    "    temp2 = temp2.loc[:, (temp2 !=0).any(axis=0)]\n",
    "    t = list(temp.columns)\n",
    "    for j in t:\n",
    "        plt.figure(figsize = (30, 20))\n",
    "        plt.subplot(4,2,1)\n",
    "        sns.distplot(temp[j], kde=True, fit=scipy.stats.norm)\n",
    "        plt.title('Units')\n",
    "        plt.subplot(4,2,2)\n",
    "        sns.distplot(temp2[j], kde=True, fit=scipy.stats.norm)\n",
    "        plt.title('Log_unit')\n",
    "        plt.subplot(4,2,3)\n",
    "        scipy.stats.probplot(temp[j], dist=\"norm\", plot=pylab)\n",
    "        plt.subplot(4,2,4)\n",
    "        scipy.stats.probplot(temp2[j], dist=\"norm\", plot=pylab)\n",
    "        pylab.show()\n",
    "        result_ks = scipy.stats.kstest(temp[j], cdf='norm')\n",
    "        result_ks_log = scipy.stats.kstest(temp2[j], cdf='norm')\n",
    "        print('Unit - test statistic: {}, p-value: {}'.format(result_ks[0], result_ks[1]))\n",
    "        print('Log Unit - test statistic: {}, p-value: {}'.format(result_ks_log[0], result_ks_log[1]))\n",
    "        print(\"Unit - Skewness: %f\" % temp[j].skew())\n",
    "        print(\"Unit - Kurtosis: %f\" % temp[j].kurt())\n",
    "        print(\"Log Unit - Skewness: %f\" % temp2[j].skew())\n",
    "        print(\"Log Unit - Kurtosis: %f\" % temp2[j].kurt())\n",
    "    # It seems like improving the normality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로그를 취하지 않은 종속변수 y값은 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로그를 취한 유닛으로 시간별 판매량을 플롯함으로써 추세, 계절성이 있는지 알아보자\n",
    "\n",
    "여기에서 UCL은 평균값에 2 Sigma를 더한값이라 하고, 범위안에 속하지 못하는 데이터는 odd하다고 가정한다. (95.45%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 2): #원래는 1~45\n",
    "    tmp = []\n",
    "    tmp = train_df[train_df['store_nbr'] == i]\n",
    "    tmp_sold = tmp[tmp['units'] > 0]\n",
    "    tmp = pd.concat([tmp[tmp['item_nbr'] == num] for num in tmp_sold['item_nbr'].unique()])\n",
    "    for j in tmp['item_nbr'].unique():\n",
    "        tmp_item = []\n",
    "        tmp_item = tmp[tmp['item_nbr'] == j]\n",
    "        tmp_item['index'] = [k for k in range(len(tmp_item))]\n",
    "        mean = tmp_item['log_units'].mean()\n",
    "        std = tmp_item['log_units'].std()\n",
    "        sig = np.sqrt(std)\n",
    "        UCL = (sig*2) + mean # 2sigma 95.45%\n",
    "        tmp_item['UCL'] = UCL\n",
    "        tmp_item_odd = tmp_item[tmp_item['log_units'] > UCL]\n",
    "        ax = tmp_item.plot(x='date', y='log_units', kind='line', figsize=(20,2), title=('{} Store, {} Item'.format(i, j)))\n",
    "        tmp_item.plot(x='date', y='UCL', kind='line', style=':', ax=ax)\n",
    "        if len(tmp_item_odd) != 0:\n",
    "            tmp_item_odd.plot(x='index', y='log_units', kind='scatter', color='r', ax=ax)\n",
    "#          plt.title('{} Store, {} Item'.format(i, j))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 oddity는 어디서 나온 것일까? 요일별로 판매량이 다른지 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_train_f1 = processed_train.pivot_table(values='units', index=['weekday'], aggfunc=np.sum)\n",
    "processed_train_f2 = processed_train[processed_train['units'] > 0].reset_index(drop=True)\n",
    "processed_train_f3 = processed_train[processed_train['log_units'] > 0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize = (25, 7))\n",
    "# unit vs weekday\n",
    "processed_train_f1.plot(kind='line', style='r', ax=axes[0])\n",
    "# axes[1].set_ylim([0,200])\n",
    "plt.xticks(rotation=0)\n",
    "axes[0].set_ylabel('Total Units')\n",
    "axes[0].set_xlabel('Weekday')\n",
    "processed_train_f2.boxplot(\"units\", \"weekday\", ax=axes[1])\n",
    "axes[1].set_ylim([0,200])\n",
    "plt.xticks(rotation=0)\n",
    "axes[1].set_ylabel('Units')\n",
    "axes[1].set_xlabel('Weekday')\n",
    "processed_train_f3.boxplot(\"log_units\", \"weekday\", ax=axes[2])\n",
    "axes[2].set_ylim([0,7.5])\n",
    "plt.xticks(rotation=0)\n",
    "axes[2].set_ylabel('Units')\n",
    "axes[2].set_xlabel('Weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유닛의 총량으로 보았을때는 차이가 분명히 나타나지만 boxplot으로 보았을때 커다란 차이점을 보기는 힘들다. (특히 유닛에 로그를 취했을 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_train = preprocessing(train_df, holiday, weather_df)\n",
    "processed_train['date'] = processed_train['date'].apply(lambda x:x.date().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 2):  #원래는 1~45\n",
    "    tmp = []\n",
    "    tmp_sold = []\n",
    "    tmp = processed_train[processed_train['store_nbr'] == i]\n",
    "    tmp_sold = tmp[tmp['log_units'] > 0]\n",
    "    tmp = pd.concat([tmp[tmp['item_nbr'] == num] for num in tmp_sold['item_nbr'].unique()])\n",
    "    for j in tmp['item_nbr'].unique():\n",
    "        tmp_item = []\n",
    "        tmp_item_odd = []\n",
    "        tmp_item_mon = []\n",
    "        tmp_item_tue = []\n",
    "        tmp_item_wed = []\n",
    "        tmp_item_thu = []\n",
    "        tmp_item_fri = []\n",
    "        tmp_item_sat = []\n",
    "        tmp_item_sun = []\n",
    "        tmp_item = tmp[tmp['item_nbr'] == j]\n",
    "        tmp_item['index'] = [k for k in range(len(tmp_item))]\n",
    "        mean = tmp_item['log_units'].mean()\n",
    "        std = tmp_item['log_units'].std()\n",
    "        sig = np.sqrt(std)\n",
    "        UCL = (sig*2) + mean # 2sigma 95.45%\n",
    "        tmp_item['UCL'] = UCL\n",
    "        tmp_item_odd = tmp_item[tmp_item['log_units'] > UCL]\n",
    "        tmp_item_mon = tmp_item_odd[tmp_item_odd['weekday'] == 0]\n",
    "        tmp_item_tue = tmp_item_odd[tmp_item_odd['weekday'] == 1]\n",
    "        tmp_item_wed = tmp_item_odd[tmp_item_odd['weekday'] == 2]\n",
    "        tmp_item_thu = tmp_item_odd[tmp_item_odd['weekday'] == 3]\n",
    "        tmp_item_fri = tmp_item_odd[tmp_item_odd['weekday'] == 4]\n",
    "        tmp_item_sat = tmp_item_odd[tmp_item_odd['weekday'] == 5]\n",
    "        tmp_item_sun = tmp_item_odd[tmp_item_odd['weekday'] == 6]\n",
    "        ax = tmp_item.plot(x='date', y='log_units', kind='line', figsize=(20,4), title=('{} Store, {} Item'.format(i, j)))\n",
    "        tmp_item.plot(x='date', y='UCL', kind='line', style=':', ax=ax)\n",
    "        if len(tmp_item_odd) != 0:\n",
    "            tmp_item_odd.plot(x='index', y='log_units', kind='scatter', color='r', alpha='0.0', ax=ax)\n",
    "            if len(tmp_item_mon) != 0:\n",
    "                tmp_item_mon.plot(x='index', y='log_units', kind='scatter', color='#FFA500', ax=ax) # orange\n",
    "                print('Monday: {}'.format(len(tmp_item_mon)))\n",
    "\n",
    "            if len(tmp_item_tue) != 0:\n",
    "                tmp_item_tue.plot(x='index', y='log_units', kind='scatter', color='#FF69B4', ax=ax) # pink\n",
    "                print('Tuesday: {}'.format(len(tmp_item_tue)))\n",
    "\n",
    "            if len(tmp_item_wed) !=0:\n",
    "                tmp_item_wed.plot(x='index', y='log_units', kind='scatter', color='y', ax=ax)\n",
    "                print('Wednsday: {}'.format(len(tmp_item_wed)))\n",
    "\n",
    "            if len(tmp_item_thu) != 0:\n",
    "                tmp_item_thu.plot(x='index', y='log_units', kind='scatter', color='g', ax=ax)\n",
    "                print('Thurday: {}'.format(len(tmp_item_thu)))\n",
    "\n",
    "            if len(tmp_item_fri) != 0:\n",
    "                tmp_item_fri.plot(x='index', y='log_units', kind='scatter', color='b', ax=ax)\n",
    "                print('Friday: {}'.format(len(tmp_item_fri)))\n",
    "\n",
    "            if len(tmp_item_sat) != 0:\n",
    "                tmp_item_sat.plot(x='index', y='log_units', kind='scatter', color='#00008B', ax=ax) # darkblue\n",
    "                print('Satday: {}'.format(len(tmp_item_sat)))\n",
    "\n",
    "            if len(tmp_item_sun) != 0:    \n",
    "                tmp_item_sun.plot(x='index', y='log_units', kind='scatter', color='m', ax=ax)            \n",
    "                print('Sunday: {}'.format(len(tmp_item_sun)))\n",
    "                           \n",
    "            plt.show()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공휴일이 미치는 영향도 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_holiday = train_df[train_df['holiday'] == True].reset_index(drop = True)\n",
    "train_df_nonholiday = train_df[train_df['holiday'] == False].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 2) : # 1 ~ 45\n",
    "    plt.subplots(figsize = (23, 400))\n",
    "    plt.subplot(90, 1, i)\n",
    "    sns.boxplot(x = 'item_nbr', y = 'log_units', data = by_store(train_df_holiday, i))\n",
    "        \n",
    "    plt.title('store_nbr = {}'.format(i))\n",
    "    \n",
    "    plt.subplot(90, 1, i + 1)\n",
    "    sns.boxplot(x = 'item_nbr', y = 'log_units', data = by_store(train_df_nonholiday, i))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_train_f4 = processed_train[processed_train['units'] > 0]\n",
    "processed_train_f5 = processed_train[processed_train['log_units'] > 0]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (10, 7))\n",
    "processed_train_f4.boxplot(\"units\", \"holiday\", ax=axes[0])\n",
    "axes[0].set_ylim([0,150])\n",
    "processed_train_f5.boxplot(\"log_units\", \"holiday\", ax=axes[1])\n",
    "axes[1].set_ylim([0,7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 이 odd한 날이 weather의 영향을 받은 날일까? \n",
    "\n",
    "문제에서 preciptotal이 1이상 snowfall이 2이상인날을 weather event, 즉 stormy weather한 날이라고 정의하였다. \n",
    "\n",
    "이 날짜에 맞춰 event가 발생하였다고 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_pivot_station_train = processed_train.pivot_table(values='event', index='date', columns='station_nbr')\n",
    "event_pivot_station_test = processed_test.pivot_table(values='event', index='date', columns='station_nbr')\n",
    "event_pivot_store_train = processed_train.pivot_table(values='event', index='date', columns='store_nbr')\n",
    "event_pivot_store_test = processed_test.pivot_table(values='event', index='date', columns='store_nbr')\n",
    "event_pivot_station_train = pd.DataFrame(event_pivot_station_train.to_records())\n",
    "event_pivot_station_test = pd.DataFrame(event_pivot_station_test.to_records())\n",
    "event_pivot_store_train = pd.DataFrame(event_pivot_store_train.to_records())\n",
    "event_pivot_store_test = pd.DataFrame(event_pivot_store_test.to_records())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 플롯한 추세, 계절성 플롯에서 odd한 값들이 event에 영향을 받은것인지 알아보기 위해 같은 플롯을 그려보자\n",
    "\n",
    "event는 초록색 odd는 빨간색 둘이 겹치는 부분은 두색의 합으로 나타난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 3): #원래는 1~45\n",
    "    tmp = []\n",
    "    tmp_item_flag = []\n",
    "    tmp = processed_train[processed_train['store_nbr'] == i]\n",
    "    tmp_sold = tmp[tmp['log_units'] > 0]\n",
    "    tmp = pd.concat([tmp[tmp['item_nbr'] == num] for num in tmp_sold['item_nbr'].unique()])\n",
    "    for j in tmp['item_nbr'].unique():\n",
    "        tmp_item = tmp[tmp['item_nbr'] == j]\n",
    "        tmp_item['index'] = [k for k in range(len(tmp_item))]\n",
    "        mean = tmp_item['log_units'].mean()\n",
    "        std = tmp_item['log_units'].std()\n",
    "        sig = np.sqrt(std)\n",
    "        UCL = (sig*2) + mean # 2sigma 95.45%\n",
    "        tmp_item['UCL'] = UCL\n",
    "        tmp_item_odd = tmp_item[tmp_item['log_units'] > UCL]\n",
    "        tmp_item_event = tmp_item[tmp_item['event'] > 0]\n",
    "        tmp_item_flag = tmp_item_event[tmp_item_event['log_units'] > UCL]\n",
    "        ax = tmp_item.plot(x='date', y='log_units', kind='line', figsize=(20,4), title=('{} Store, {} Item'.format(i, j)))\n",
    "        tmp_item.plot(x='date', y='UCL', kind='line', style=':', ax=ax)\n",
    "        if len(tmp_item_odd) != 0:\n",
    "            tmp_item_event.plot(x='index', y='log_units', kind='scatter', color='g', ax=ax)\n",
    "            tmp_item_odd.plot(x='index', y='log_units', kind='scatter', color='r', ax=ax)\n",
    "            if len(tmp_item_flag) !=0:\n",
    "                tmp_item_flag.plot(x='index', y='log_units', kind='scatter', color='c', ax=ax)\n",
    "                print('Warning! : {}, Match: {}'.format(tmp_item_flag['date'], len(tmp_item_flag)))\n",
    "        else:\n",
    "            tmp_item_event.plot(x='index', y='log_units', kind='scatter', color='g', ax=ax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "event와 odd가 겹쳐질때 warning을 주도록 하였는데 겹치는 경우가 희소하다. \n",
    "\n",
    "그렇다면 다른 weather정보가 unit에 영향을 미치는지 알아보도록 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_tmp = weather[weather['date'] <= '2013-03-31'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_col_ls = list(weather_tmp.columns)\n",
    "for col in w_col_ls:\n",
    "    if col == 'date':\n",
    "        weather_tmp[col] = weather_tmp[col]\n",
    "    elif col == 'sunrise':\n",
    "        weather_tmp[col] = weather_tmp[col]\n",
    "    elif col == 'sunset':\n",
    "        weather_tmp[col] = weather_tmp[col]\n",
    "    elif col == 'codesum':\n",
    "        weather_tmp[col] = weather_tmp[col]\n",
    "    else:\n",
    "        weather_tmp[col] = TM_transform(weather_tmp[col], 0.001, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_tmp = weather_tmp.replace('M', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_tmp.drop(['RA_flag', 'FZ_flag', 'FG_flag', 'BR_flag', 'normal_flag',\n",
    "       'UP_flag', 'MI_flag', 'SN_flag', 'HZ_flag', 'TS_flag', 'VC_flag',\n",
    "       'DZ_flag', 'BL_flag', 'BC_flag', 'DU_flag', 'SQ_flag', 'PL_flag',\n",
    "       'FU_flag', 'GR_flag', 'GS_flag', 'SG_flag', 'PR_flag', 'snow_event',\n",
    "       'rain_event'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weather_tmp['date'] = weather_tmp['date'].apply(lambda x:x.date().strftime('%Y-%m-%d'))\n",
    "# weather_tmp = weather_tmp.dropna(how='any', axis=0)\n",
    "# weather_tmp = weather_tmp.astype('float')\n",
    "weather_tmp = weather_tmp.reset_index(drop=True)\n",
    "weather_tmp['station_nbr'] = weather_tmp['station_nbr'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_nbr in range(1, 2) : # 1~20번 station\n",
    "    weather_tmp_st = weather_tmp[weather_tmp['station_nbr'] == station_nbr].reset_index(drop = True)\n",
    "    train_df_ = pd.merge(train_df, key, on = 'store_nbr')\n",
    "    train_df_st = train_df_[train_df_['station_nbr'] == station_nbr]\n",
    "    \n",
    "    nonzero = train_df_st[train_df_st['log_units'] > 0]\n",
    "    item_nbr_list = list(nonzero['item_nbr'].unique())\n",
    "    \n",
    "    for item_nbr in item_nbr_list :\n",
    "        train_df_st_it = train_df_st[train_df_st['item_nbr'] == item_nbr]\n",
    "        weather_tmp_st_it = pd.merge(weather_tmp_st, train_df_st_it, on = ['date', 'station_nbr'])\n",
    "        \n",
    "        sns.pairplot(weather_tmp_st_it, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb'],\n",
    "                     y_vars=['units', 'log_units'], kind='reg')\n",
    "        plt.show()\n",
    "\n",
    "        sns.pairplot(weather_tmp_st_it, x_vars=['heat', 'cool', 'preciptotal', 'stnpressure', 'resultspeed'],\n",
    "                     y_vars=['units', 'log_units'], kind='reg')\n",
    "        plt.show()\n",
    "\n",
    "        sns.pairplot(weather_tmp_st_it, x_vars=['avgspeed', 'weekday', 'weekend', 'holiday', 'weekday_holiday', 'weekend_holiday'],\n",
    "                     y_vars=['units', 'log_units'], kind='reg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weather column간 관계\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['tmax'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['tmin'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['tavg'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['dewpoint'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['wetbulb'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['heat'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['cool'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['preciptotal'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['stnpressure'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['resultspeed'], kind='reg')\n",
    "plt.show()\n",
    "sns.pairplot(weather_tmp, x_vars=['tmax', 'tmin', 'tavg', 'dewpoint', 'wetbulb', 'heat', 'cool',\n",
    "       'preciptotal', 'stnpressure', 'resultspeed', 'avgspeed'], y_vars=['avgspeed'], kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature와 dewpoint wetbulb, cool과 heat외에는 상관관계가 없어보인다. 상관계수를 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['log_units'] = np.log(train_df['units'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.merge(train_df, key, on = 'store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = tmp[tmp['units'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 weather station별로 units이 0이 아닌 item_nbr들의 units와 weather column들 간의 correlation을 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weather_tmp.drop(['RA_flag', 'FZ_flag', 'FG_flag', 'BR_flag', 'normal_flag',\n",
    "#        'UP_flag', 'MI_flag', 'SN_flag', 'HZ_flag', 'TS_flag', 'VC_flag',\n",
    "#        'DZ_flag', 'BL_flag', 'BC_flag', 'DU_flag', 'SQ_flag', 'PL_flag',\n",
    "#        'FU_flag', 'GR_flag', 'GS_flag', 'SG_flag', 'PR_flag', 'snow_event',\n",
    "#        'rain_event'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1, 2) : #station_nbr은 1~20까지....\n",
    "    weather_cor = weather_tmp[weather_tmp['station_nbr'] == x]\n",
    "    tmp_ = tmp[tmp['station_nbr'] == x]\n",
    "    weather_cor1 = pd.merge(weather_cor, tmp_, on = 'station_nbr')\n",
    "    item_nbr_list = list(weather_cor1['item_nbr'].unique())\n",
    "\n",
    "    for num in item_nbr_list :\n",
    "        weather_cor_ = weather_cor1[weather_cor1['item_nbr'] == num]\n",
    "        plt.figure(figsize=(20,15))\n",
    "        sns.heatmap(weather_cor_.corr(), annot = True, fmt = '.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 관심있는 units와 log_units는 weather와는 별로 연관성이 없어보인다. 어떻게 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 보았다시피 각 스토어, 각 아이템별로 나누어 모델링을 해야함은 분명하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weather와 log_units(또는 units)의 큰 상관관계는 없는 것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그나마 weekday와 holiday가 log_units(또는 units)에 영향을 미치는 것으로 보인다(item_nbr에 따라 다름)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contrast_weekday = np.eye(7)\n",
    "result_test = [] #outlier 제거 전\n",
    "result_test2 = [] #outlier 제거 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_train.drop(['weekend', 'weekday_holiday', 'weekend_holiday',\n",
    "       'station_nbr', 'tmax', 'tmin', 'tavg', 'depart', 'dewpoint', 'wetbulb',\n",
    "       'heat', 'cool', 'sunrise', 'sunset', 'codesum', 'snowfall',\n",
    "       'preciptotal', 'stnpressure', 'sealevel', 'resultspeed', 'resultspeed',\n",
    "       'resultdir', 'avgspeed'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_test.drop(['weekend', 'weekday_holiday', 'weekend_holiday', 'station_nbr', 'tmax', 'tmin',\n",
    "       'tavg', 'depart', 'dewpoint', 'wetbulb', 'heat', 'cool', 'sunrise',\n",
    "       'sunset', 'codesum', 'snowfall', 'preciptotal', 'stnpressure',\n",
    "       'sealevel', 'resultspeed', 'resultspeed', 'resultdir', 'avgspeed'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>units</th>\n",
       "      <th>log_units</th>\n",
       "      <th>weekday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2255848</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>35</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255849</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>35</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255850</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>35</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255851</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>35</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255852</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>35</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  store_nbr  item_nbr  units  log_units  weekday  holiday  \\\n",
       "2255848  2013-03-31         35       107      0        0.0        6     True   \n",
       "2255849  2013-03-31         35       108      0        0.0        6     True   \n",
       "2255850  2013-03-31         35       109      0        0.0        6     True   \n",
       "2255851  2013-03-31         35       110      0        0.0        6     True   \n",
       "2255852  2013-03-31         35       111      0        0.0        6     True   \n",
       "\n",
       "         event  \n",
       "2255848      0  \n",
       "2255849      0  \n",
       "2255850      0  \n",
       "2255851      0  \n",
       "2255852      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6bccd79a3c19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_formula\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_units ~ C(store_nbr):C(item_nbr):C(weekday, contrast_weekday) + 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[1;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[1;32m--> 155\u001b[1;33m                                   missing=missing)\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\statsmodels\\formula\\formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[1;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[1;32m---> 65\u001b[1;33m                                NA_action=na_action)\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[1;32m--> 310\u001b[1;33m                                       NA_action, return_type)\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPatsyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model is missing required outcome variables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    167\u001b[0m         return build_design_matrices(design_infos, data,\n\u001b[0;32m    168\u001b[0m                                      \u001b[0mNA_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNA_action\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                                      return_type=return_type)\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# No builders, but maybe we can still get matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\patsy\\build.py\u001b[0m in \u001b[0;36mbuild_design_matrices\u001b[1;34m(design_infos, data, NA_action, return_type, dtype)\u001b[0m\n\u001b[0;32m    931\u001b[0m         results.append(_build_design_matrix(design_info,\n\u001b[0;32m    932\u001b[0m                                             \u001b[0mfactor_info_to_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                                             dtype))\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[0mmatrices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mneed_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\patsy\\build.py\u001b[0m in \u001b[0;36m_build_design_matrix\u001b[1;34m(design_info, factor_info_to_values, dtype)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[0mneed_reshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesign_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDesignMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m     \u001b[0mstart_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubterms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesign_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterm_codings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = sm.OLS.from_formula('log_units ~ C(store_nbr):C(item_nbr):C(weekday, contrast_weekday) + 0', data = processed_train)\n",
    "result_model = model.fit()\n",
    "print(result_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = processed_train[processed_train['store_nbr'] == 1].reset_index(drop = True)\n",
    "\n",
    "model1 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_1)\n",
    "result_model1 = model1.fit()\n",
    "print(result_model1.summary())\n",
    "\n",
    "test_1 = processed_test[processed_test['store_nbr'] == 1]\n",
    "\n",
    "test_1['log_units'] = result_model1.predict(test_1)\n",
    "test_1['units'] = np.exp(test_1['log_units']) - 1\n",
    "result_test.append(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv1_1 = df_1[:10101]\n",
    "cv1_2 = df_1[10101:20202]\n",
    "cv1_3 = df_1[20202:30414]\n",
    "cv1_4 = df_1[30414:40515]\n",
    "cv1_5 = df_1[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv1_train = pd.concat([cv1_1, cv1_2, cv1_3, cv1_4], ignore_index = True)\n",
    "cv1_test = cv1_5\n",
    "\n",
    "model_cv1 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv1_train)\n",
    "result_model_cv1 = model_cv1.fit()\n",
    "print(result_model_cv1.summary())\n",
    "\n",
    "cv1_test['log_units_cv'] = result_model_cv1.predict(cv1_test)\n",
    "cv1_test['units_cv'] = np.exp(cv1_test['log_units_cv']) - 1\n",
    "\n",
    "cv1_test['RMSLE'] = np.log((cv1_test['units_cv'] + 1) / (cv1_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv1_test['RMSLE']) / len(cv1_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_1 = get_item_nbr(df_1) #1번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence1 = result_model1.get_influence()\n",
    "\n",
    "cooks_d2_1, pvals1 = influence1.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr1 = 4 / (len(df_1) - 778)\n",
    "idx1 = np.where(cooks_d2_1 > fox_cr1)[0]\n",
    "\n",
    "print(len(idx1)) #outlier 갯수\n",
    "\n",
    "X1 = result_model1.predict(df_1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X1, df_1['log_units'])\n",
    "plt.scatter(X1[idx1], df_1.loc[idx1]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx1)), idx1, list(zip(X1[idx1], df_1.loc[idx1]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx1), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in idx1 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_1 :\n",
    "        if item_nbr == df_1.loc[num].item_nbr :\n",
    "            mean = df_1[df_1['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_1.set_value(num, 'log_units', mean)\n",
    "            df_1.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model1_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_1)\n",
    "result_model1_new = model1_new.fit()\n",
    "print(result_model1_new.summary())\n",
    "\n",
    "test_1_new = processed_test[processed_test['store_nbr'] == 1]\n",
    "\n",
    "test_1_new['log_units'] = result_model1_new.predict(test_1_new)\n",
    "test_1_new['units'] = np.exp(test_1_new['log_units']) - 1\n",
    "result_test2.append(test_1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = processed_train[processed_train['store_nbr'] == 2].reset_index(drop = True)\n",
    "\n",
    "model2 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_2)\n",
    "result_model2 = model2.fit()\n",
    "print(result_model2.summary())\n",
    "\n",
    "test_2 = processed_test[processed_test['store_nbr'] == 2]\n",
    "\n",
    "test_2['log_units'] = result_model2.predict(test_2)\n",
    "test_2['units'] = np.exp(test_2['log_units']) - 1\n",
    "result_test.append(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv2_1 = df_2[:10101]\n",
    "cv2_2 = df_2[10101:20202]\n",
    "cv2_3 = df_2[20202:30414]\n",
    "cv2_4 = df_2[30414:40515]\n",
    "cv2_5 = df_2[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv2_train = pd.concat([cv2_1, cv2_2, cv2_3, cv2_4], ignore_index = True)\n",
    "cv2_test = cv2_5\n",
    "\n",
    "model_cv2 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv2_train)\n",
    "result_model_cv2 = model_cv2.fit()\n",
    "print(result_model_cv2.summary())\n",
    "\n",
    "cv2_test['log_units_cv'] = result_model_cv2.predict(cv2_test)\n",
    "cv2_test['units_cv'] = np.exp(cv2_test['log_units_cv']) - 1\n",
    "\n",
    "cv2_test['RMSLE'] = np.log((cv2_test['units_cv'] + 1) / (cv2_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv2_test['RMSLE']) / len(cv2_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_2 = get_item_nbr(df_2) #2번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence2 = result_model2.get_influence()\n",
    "\n",
    "cooks_d2_2, pvals2 = influence2.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr2 = 4 / (len(df_2) - 778)\n",
    "idx2 = np.where(cooks_d2_2 > fox_cr2)[0]\n",
    "\n",
    "print(len(idx2)) #outlier 갯수\n",
    "\n",
    "X2 = result_model2.predict(df_2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X2, df_2['log_units'])\n",
    "plt.scatter(X2[idx2], df_2.loc[idx2]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx2)), idx2, list(zip(X2[idx2], df_2.loc[idx2]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx2), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in idx2 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_2 :\n",
    "        if item_nbr == df_2.loc[num].item_nbr :\n",
    "            mean = df_2[df_2['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_2.set_value(num, 'log_units', mean)\n",
    "            df_2.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model2_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_2)\n",
    "result_model2_new = model2_new.fit()\n",
    "print(result_model2_new.summary())\n",
    "\n",
    "test_2_new = processed_test[processed_test['store_nbr'] == 2]\n",
    "\n",
    "test_2_new['log_units'] = result_model2_new.predict(test_2_new)\n",
    "test_2_new['units'] = np.exp(test_2_new['log_units']) - 1\n",
    "result_test2.append(test_2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = processed_train[processed_train['store_nbr'] == 3].reset_index(drop = True)\n",
    "\n",
    "model3 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_3)\n",
    "result_model3 = model3.fit()\n",
    "print(result_model3.summary())\n",
    "\n",
    "test_3 = processed_test[processed_test['store_nbr'] == 3]\n",
    "\n",
    "test_3['log_units'] = result_model3.predict(test_3)\n",
    "test_3['units'] = np.exp(test_3['log_units']) - 1\n",
    "result_test.append(test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv3_1 = df_3[:10101]\n",
    "cv3_2 = df_3[10101:20202]\n",
    "cv3_3 = df_3[20202:30414]\n",
    "cv3_4 = df_3[30414:40515]\n",
    "cv3_5 = df_3[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv3_train = pd.concat([cv3_1, cv3_2, cv3_3, cv3_4], ignore_index = True)\n",
    "cv3_test = cv3_5\n",
    "\n",
    "model_cv3 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv3_train)\n",
    "result_model_cv3 = model_cv3.fit()\n",
    "print(result_model_cv3.summary())\n",
    "\n",
    "cv3_test['log_units_cv'] = result_model_cv3.predict(cv3_test)\n",
    "cv3_test['units_cv'] = np.exp(cv3_test['log_units_cv']) - 1\n",
    "\n",
    "cv3_test['RMSLE'] = np.log((cv3_test['units_cv'] + 1) / (cv3_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv3_test['RMSLE']) / len(cv3_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_3 = get_item_nbr(df_3) #3번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence3 = result_model3.get_influence()\n",
    "\n",
    "cooks_d2_3, pvals3 = influence3.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr3 = 4 / (len(df_3) - 778)\n",
    "idx3 = np.where(cooks_d2_3 > fox_cr3)[0]\n",
    "\n",
    "print(len(idx3)) #outlier 갯수\n",
    "\n",
    "X3 = result_model3.predict(df_3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X3, df_3['log_units'])\n",
    "plt.scatter(X3[idx3], df_3.loc[idx3]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx3)), idx3, list(zip(X3[idx3], df_3.loc[idx3]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx3), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in idx3 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_3 :\n",
    "        if item_nbr == df_3.loc[num].item_nbr :\n",
    "            mean = df_3[df_3['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_3.set_value(num, 'log_units', mean)\n",
    "            df_3.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model3_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_3)\n",
    "result_model3_new = model3_new.fit()\n",
    "print(result_model3_new.summary())\n",
    "\n",
    "test_3_new = processed_test[processed_test['store_nbr'] == 3]\n",
    "\n",
    "test_3_new['log_units'] = result_model3_new.predict(test_3_new)\n",
    "test_3_new['units'] = np.exp(test_3_new['log_units']) - 1\n",
    "result_test2.append(test_3_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_4 = processed_train[processed_train['store_nbr'] == 4].reset_index(drop = True)\n",
    "\n",
    "model4 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_4)\n",
    "result_model4 = model4.fit()\n",
    "print(result_model4.summary())\n",
    "\n",
    "test_4 = processed_test[processed_test['store_nbr'] == 4]\n",
    "\n",
    "test_4['log_units'] = result_model4.predict(test_4)\n",
    "test_4['units'] = np.exp(test_4['log_units']) - 1\n",
    "result_test.append(test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv4_1 = df_4[:10101]\n",
    "cv4_2 = df_4[10101:20202]\n",
    "cv4_3 = df_4[20202:30414]\n",
    "cv4_4 = df_4[30414:40515]\n",
    "cv4_5 = df_4[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv4_train = pd.concat([cv4_1, cv4_2, cv4_3, cv4_4], ignore_index = True)\n",
    "cv4_test = cv4_5\n",
    "\n",
    "model_cv4 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv4_train)\n",
    "result_model_cv4 = model_cv4.fit()\n",
    "print(result_model_cv4.summary())\n",
    "\n",
    "cv4_test['log_units_cv'] = result_model_cv4.predict(cv4_test)\n",
    "cv4_test['units_cv'] = np.exp(cv4_test['log_units_cv']) - 1\n",
    "\n",
    "cv4_test['RMSLE'] = np.log((cv4_test['units_cv'] + 1) / (cv4_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv4_test['RMSLE']) / len(cv4_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_4 = get_item_nbr(df_4) #4번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence4 = result_model4.get_influence()\n",
    "\n",
    "cooks_d2_4, pvals4 = influence4.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr4 = 4 / (len(df_4) - 778)\n",
    "idx4 = np.where(cooks_d2_4 > fox_cr4)[0]\n",
    "\n",
    "print(len(idx4)) #outlier 갯수\n",
    "\n",
    "X4 = result_model4.predict(df_4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X4, df_4['log_units'])\n",
    "plt.scatter(X4[idx4], df_4.loc[idx4]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx4)), idx4, list(zip(X4[idx4], df_4.loc[idx4]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx4), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx4 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_4 :\n",
    "        if item_nbr == df_4.loc[num].item_nbr :\n",
    "            mean = df_4[df_4['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_4.set_value(num, 'log_units', mean)\n",
    "            df_4.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model4_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_4)\n",
    "result_model4_new = model4_new.fit()\n",
    "print(result_model4_new.summary())\n",
    "\n",
    "test_4_new = processed_test[processed_test['store_nbr'] == 4]\n",
    "\n",
    "test_4_new['log_units'] = result_model4_new.predict(test_4_new)\n",
    "test_4_new['units'] = np.exp(test_4_new['log_units']) - 1\n",
    "result_test2.append(test_4_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_5 = processed_train[processed_train['store_nbr'] == 5].reset_index(drop = True)\n",
    "\n",
    "model5 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_5)\n",
    "result_model5 = model5.fit()\n",
    "print(result_model5.summary())\n",
    "\n",
    "test_5 = processed_test[processed_test['store_nbr'] == 5]\n",
    "\n",
    "test_5['log_units'] = result_model5.predict(test_5)\n",
    "test_5['units'] = np.exp(test_5['log_units']) - 1\n",
    "result_test.append(test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv5_1 = df_5[:10101]\n",
    "cv5_2 = df_5[10101:20202]\n",
    "cv5_3 = df_5[20202:30414]\n",
    "cv5_4 = df_5[30414:40515]\n",
    "cv5_5 = df_5[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv5_train = pd.concat([cv5_1, cv5_2, cv5_3, cv5_4], ignore_index = True)\n",
    "cv5_test = cv5_5\n",
    "\n",
    "model_cv5 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv5_train)\n",
    "result_model_cv5 = model_cv5.fit()\n",
    "print(result_model_cv5.summary())\n",
    "\n",
    "cv5_test['log_units_cv'] = result_model_cv5.predict(cv5_test)\n",
    "cv5_test['units_cv'] = np.exp(cv5_test['log_units_cv']) - 1\n",
    "\n",
    "cv5_test['RMSLE'] = np.log((cv5_test['units_cv'] + 1) / (cv5_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv5_test['RMSLE']) / len(cv5_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_5 = get_item_nbr(df_5) #5번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence5 = result_model5.get_influence()\n",
    "\n",
    "cooks_d2_5, pvals5 = influence5.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr5 = 4 / (len(df_5) - 778)\n",
    "idx5 = np.where(cooks_d2_5 > fox_cr5)[0]\n",
    "\n",
    "print(len(idx5)) #outlier 갯수\n",
    "\n",
    "X5 = result_model5.predict(df_5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X5, df_5['log_units'])\n",
    "plt.scatter(X5[idx5], df_5.loc[idx5]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx5)), idx5, list(zip(X5[idx5], df_5.loc[idx5]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx5), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx5 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_5 :\n",
    "        if item_nbr == df_5.loc[num].item_nbr :\n",
    "            mean = df_5[df_5['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_5.set_value(num, 'log_units', mean)\n",
    "            df_5.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model5_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_5)\n",
    "result_model5_new = model5_new.fit()\n",
    "print(result_model5_new.summary())\n",
    "\n",
    "test_5_new = processed_test[processed_test['store_nbr'] == 5]\n",
    "\n",
    "test_5_new['log_units'] = result_model5_new.predict(test_5_new)\n",
    "test_5_new['units'] = np.exp(test_5_new['log_units']) - 1\n",
    "result_test2.append(test_5_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_6 = processed_train[processed_train['store_nbr'] == 6].reset_index(drop = True)\n",
    "\n",
    "model6 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_6)\n",
    "result_model6 = model6.fit()\n",
    "print(result_model6.summary())\n",
    "\n",
    "test_6 = processed_test[processed_test['store_nbr'] == 6]\n",
    "\n",
    "test_6['log_units'] = result_model6.predict(test_6)\n",
    "test_6['units'] = np.exp(test_6['log_units']) - 1\n",
    "result_test.append(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv6_1 = df_6[:10101]\n",
    "cv6_2 = df_6[10101:20202]\n",
    "cv6_3 = df_6[20202:30414]\n",
    "cv6_4 = df_6[30414:40515]\n",
    "cv6_5 = df_6[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv6_train = pd.concat([cv6_1, cv6_2, cv6_3, cv6_4], ignore_index = True)\n",
    "cv6_test = cv6_5\n",
    "\n",
    "model_cv6 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv6_train)\n",
    "result_model_cv6 = model_cv6.fit()\n",
    "print(result_model_cv6.summary())\n",
    "\n",
    "cv6_test['log_units_cv'] = result_model_cv6.predict(cv6_test)\n",
    "cv6_test['units_cv'] = np.exp(cv6_test['log_units_cv']) - 1\n",
    "\n",
    "cv6_test['RMSLE'] = np.log((cv6_test['units_cv'] + 1) / (cv6_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv6_test['RMSLE']) / len(cv6_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_6 = get_item_nbr(df_6) #6번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence6 = result_model6.get_influence()\n",
    "\n",
    "cooks_d2_6, pvals6 = influence6.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr6 = 4 / (len(df_6) - 778)\n",
    "idx6 = np.where(cooks_d2_6 > fox_cr6)[0]\n",
    "\n",
    "print(len(idx6)) #outlier 갯수\n",
    "\n",
    "X6 = result_model6.predict(df_6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X6, df_6['log_units'])\n",
    "plt.scatter(X6[idx6], df_6.loc[idx6]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx6)), idx6, list(zip(X6[idx6], df_6.loc[idx6]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx6), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx6 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_6 :\n",
    "        if item_nbr == df_6.loc[num].item_nbr :\n",
    "            mean = df_6[df_6['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_6.set_value(num, 'log_units', mean)\n",
    "            df_6.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model6_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_6)\n",
    "result_model6_new = model6_new.fit()\n",
    "print(result_model6_new.summary())\n",
    "\n",
    "test_6_new = processed_test[processed_test['store_nbr'] == 6]\n",
    "\n",
    "test_6_new['log_units'] = result_model6_new.predict(test_6_new)\n",
    "test_6_new['units'] = np.exp(test_6_new['log_units']) - 1\n",
    "result_test2.append(test_6_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_7 = processed_train[processed_train['store_nbr'] == 7].reset_index(drop = True)\n",
    "\n",
    "model7 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_7)\n",
    "result_model7 = model7.fit()\n",
    "print(result_model7.summary())\n",
    "\n",
    "test_7 = processed_test[processed_test['store_nbr'] == 7]\n",
    "\n",
    "test_7['log_units'] = result_model7.predict(test_7)\n",
    "test_7['units'] = np.exp(test_7['log_units']) - 1\n",
    "result_test.append(test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv7_1 = df_7[:10101]\n",
    "cv7_2 = df_7[10101:20202]\n",
    "cv7_3 = df_7[20202:30414]\n",
    "cv7_4 = df_7[30414:40515]\n",
    "cv7_5 = df_7[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv7_train = pd.concat([cv7_1, cv7_2, cv7_3, cv7_4], ignore_index = True)\n",
    "cv7_test = cv7_5\n",
    "\n",
    "model_cv7 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv7_train)\n",
    "result_model_cv7 = model_cv7.fit()\n",
    "print(result_model_cv7.summary())\n",
    "\n",
    "cv7_test['log_units_cv'] = result_model_cv7.predict(cv7_test)\n",
    "cv7_test['units_cv'] = np.exp(cv7_test['log_units_cv']) - 1\n",
    "\n",
    "cv7_test['RMSLE'] = np.log((cv7_test['units_cv'] + 1) / (cv7_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv7_test['RMSLE']) / len(cv7_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_7 = get_item_nbr(df_7) #7번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence7 = result_model7.get_influence()\n",
    "\n",
    "cooks_d2_7, pvals7 = influence7.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr7 = 4 / (len(df_7) - 778)\n",
    "idx7 = np.where(cooks_d2_7 > fox_cr7)[0]\n",
    "\n",
    "print(len(idx7)) #outlier 갯수\n",
    "\n",
    "X7 = result_model7.predict(df_7)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X7, df_7['log_units'])\n",
    "plt.scatter(X7[idx7], df_7.loc[idx7]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx7)), idx7, list(zip(X7[idx7], df_7.loc[idx7]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx7), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx7 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_7 :\n",
    "        if item_nbr == df_7.loc[num].item_nbr :\n",
    "            mean = df_7[df_7['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_7.set_value(num, 'log_units', mean)\n",
    "            df_7.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model7_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_7)\n",
    "result_model7_new = model7_new.fit()\n",
    "print(result_model7_new.summary())\n",
    "\n",
    "test_7_new = processed_test[processed_test['store_nbr'] == 7]\n",
    "\n",
    "test_7_new['log_units'] = result_model7_new.predict(test_7_new)\n",
    "test_7_new['units'] = np.exp(test_7_new['log_units']) - 1\n",
    "result_test2.append(test_7_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_8 = processed_train[processed_train['store_nbr'] == 8].reset_index(drop = True)\n",
    "\n",
    "model8 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_8)\n",
    "result_model8 = model8.fit()\n",
    "print(result_model8.summary())\n",
    "\n",
    "test_8 = processed_test[processed_test['store_nbr'] == 8]\n",
    "\n",
    "test_8['log_units'] = result_model8.predict(test_8)\n",
    "test_8['units'] = np.exp(test_8['log_units']) - 1\n",
    "result_test.append(test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv8_1 = df_8[:10101]\n",
    "cv8_2 = df_8[10101:20202]\n",
    "cv8_3 = df_8[20202:30414]\n",
    "cv8_4 = df_8[30414:40515]\n",
    "cv8_5 = df_8[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv8_train = pd.concat([cv8_1, cv8_2, cv8_3, cv8_4], ignore_index = True)\n",
    "cv8_test = cv8_5\n",
    "\n",
    "model_cv8 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv8_train)\n",
    "result_model_cv8 = model_cv8.fit()\n",
    "print(result_model_cv8.summary())\n",
    "\n",
    "cv8_test['log_units_cv'] = result_model_cv8.predict(cv8_test)\n",
    "cv8_test['units_cv'] = np.exp(cv8_test['log_units_cv']) - 1\n",
    "\n",
    "cv8_test['RMSLE'] = np.log((cv8_test['units_cv'] + 1) / (cv8_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv8_test['RMSLE']) / len(cv8_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_8 = get_item_nbr(df_8) #8번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence8 = result_model8.get_influence()\n",
    "\n",
    "cooks_d2_8, pvals8 = influence8.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr8 = 4 / (len(df_8) - 778)\n",
    "idx8 = np.where(cooks_d2_8 > fox_cr8)[0]\n",
    "\n",
    "print(len(idx8)) #outlier 갯수\n",
    "\n",
    "X8 = result_model8.predict(df_8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X8, df_8['log_units'])\n",
    "plt.scatter(X8[idx8], df_8.loc[idx8]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx8)), idx8, list(zip(X8[idx8], df_8.loc[idx8]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx8), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx8 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_8 :\n",
    "        if item_nbr == df_8.loc[num].item_nbr :\n",
    "            mean = df_8[df_8['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_8.set_value(num, 'log_units', mean)\n",
    "            df_8.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model8_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_8)\n",
    "result_model8_new = model8_new.fit()\n",
    "print(result_model8_new.summary())\n",
    "\n",
    "test_8_new = processed_test[processed_test['store_nbr'] == 8]\n",
    "\n",
    "test_8_new['log_units'] = result_model8_new.predict(test_8_new)\n",
    "test_8_new['units'] = np.exp(test_8_new['log_units']) - 1\n",
    "result_test2.append(test_8_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_9 = processed_train[processed_train['store_nbr'] == 9].reset_index(drop = True)\n",
    "\n",
    "model9 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_9)\n",
    "result_model9 = model9.fit()\n",
    "print(result_model9.summary())\n",
    "\n",
    "test_9 = processed_test[processed_test['store_nbr'] == 9]\n",
    "\n",
    "test_9['log_units'] = result_model9.predict(test_9)\n",
    "test_9['units'] = np.exp(test_9['log_units']) - 1\n",
    "result_test.append(test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv9_1 = df_9[:10101]\n",
    "cv9_2 = df_9[10101:20202]\n",
    "cv9_3 = df_9[20202:30414]\n",
    "cv9_4 = df_9[30414:40515]\n",
    "cv9_5 = df_9[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv9_train = pd.concat([cv9_1, cv9_2, cv9_3, cv9_4], ignore_index = True)\n",
    "cv9_test = cv9_5\n",
    "\n",
    "model_cv9 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv9_train)\n",
    "result_model_cv9 = model_cv9.fit()\n",
    "print(result_model_cv9.summary())\n",
    "\n",
    "cv9_test['log_units_cv'] = result_model_cv9.predict(cv9_test)\n",
    "cv9_test['units_cv'] = np.exp(cv9_test['log_units_cv']) - 1\n",
    "\n",
    "cv9_test['RMSLE'] = np.log((cv9_test['units_cv'] + 1) / (cv9_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv9_test['RMSLE']) / len(cv9_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_9 = get_item_nbr(df_9) #9번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence9 = result_model9.get_influence()\n",
    "\n",
    "cooks_d2_9, pvals9 = influence9.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr9 = 4 / (len(df_9) - 778)\n",
    "idx9 = np.where(cooks_d2_9 > fox_cr9)[0]\n",
    "\n",
    "print(len(idx9)) #outlier 갯수\n",
    "\n",
    "X9 = result_model9.predict(df_9)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X9, df_9['log_units'])\n",
    "plt.scatter(X9[idx9], df_9.loc[idx9]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx9)), idx9, list(zip(X9[idx9], df_9.loc[idx9]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx9), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx9 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_9 :\n",
    "        if item_nbr == df_9.loc[num].item_nbr :\n",
    "            mean = df_9[df_9['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_9.set_value(num, 'log_units', mean)\n",
    "            df_9.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model9_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_9)\n",
    "result_model9_new = model9_new.fit()\n",
    "print(result_model9_new.summary())\n",
    "\n",
    "test_9_new = processed_test[processed_test['store_nbr'] == 9]\n",
    "\n",
    "test_9_new['log_units'] = result_model9_new.predict(test_9_new)\n",
    "test_9_new['units'] = np.exp(test_9_new['log_units']) - 1\n",
    "result_test2.append(test_9_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_10 = processed_train[processed_train['store_nbr'] == 10].reset_index(drop = True)\n",
    "\n",
    "model10 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_10)\n",
    "result_model10 = model10.fit()\n",
    "print(result_model10.summary())\n",
    "\n",
    "test_10 = processed_test[processed_test['store_nbr'] == 10]\n",
    "\n",
    "test_10['log_units'] = result_model10.predict(test_10)\n",
    "test_10['units'] = np.exp(test_10['log_units']) - 1\n",
    "result_test.append(test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv10_1 = df_10[:10101]\n",
    "cv10_2 = df_10[10101:20202]\n",
    "cv10_3 = df_10[20202:30414]\n",
    "cv10_4 = df_10[30414:40515]\n",
    "cv10_5 = df_10[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv10_train = pd.concat([cv10_1, cv10_2, cv10_3, cv10_4], ignore_index = True)\n",
    "cv10_test = cv10_5\n",
    "\n",
    "model_cv10 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv10_train)\n",
    "result_model_cv10 = model_cv10.fit()\n",
    "print(result_model_cv10.summary())\n",
    "\n",
    "cv10_test['log_units_cv'] = result_model_cv10.predict(cv10_test)\n",
    "cv10_test['units_cv'] = np.exp(cv10_test['log_units_cv']) - 1\n",
    "\n",
    "cv10_test['RMSLE'] = np.log((cv10_test['units_cv'] + 1) / (cv10_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv10_test['RMSLE']) / len(cv10_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_10 = get_item_nbr(df_10) #10번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence10 = result_model10.get_influence()\n",
    "\n",
    "cooks_d2_10, pvals10 = influence10.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr10 = 4 / (len(df_10) - 778)\n",
    "idx10 = np.where(cooks_d2_10 > fox_cr10)[0]\n",
    "\n",
    "print(len(idx10)) #outlier 갯수\n",
    "\n",
    "X10 = result_model10.predict(df_10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X10, df_10['log_units'])\n",
    "plt.scatter(X10[idx10], df_10.loc[idx10]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx10)), idx10, list(zip(X10[idx10], df_10.loc[idx10]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx10), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx10 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_10 :\n",
    "        if item_nbr == df_10.loc[num].item_nbr :\n",
    "            mean = df_10[df_10['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_10.set_value(num, 'log_units', mean)\n",
    "            df_10.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model10_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_10)\n",
    "result_model10_new = model10_new.fit()\n",
    "print(result_model10_new.summary())\n",
    "\n",
    "test_10_new = processed_test[processed_test['store_nbr'] == 10]\n",
    "\n",
    "test_10_new['log_units'] = result_model10_new.predict(test_10_new)\n",
    "test_10_new['units'] = np.exp(test_10_new['log_units']) - 1\n",
    "result_test2.append(test_10_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_11 = processed_train[processed_train['store_nbr'] == 11].reset_index(drop = True)\n",
    "\n",
    "model11 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_11)\n",
    "result_model11 = model11.fit()\n",
    "print(result_model11.summary())\n",
    "\n",
    "test_11 = processed_test[processed_test['store_nbr'] == 11]\n",
    "\n",
    "test_11['log_units'] = result_model11.predict(test_11)\n",
    "test_11['units'] = np.exp(test_11['log_units']) - 1\n",
    "result_test.append(test_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv11_1 = df_11[:10101]\n",
    "cv11_2 = df_11[10101:20202]\n",
    "cv11_3 = df_11[20202:30414]\n",
    "cv11_4 = df_11[30414:40515]\n",
    "cv11_5 = df_11[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv11_train = pd.concat([cv11_1, cv11_2, cv11_3, cv11_4], ignore_index = True)\n",
    "cv11_test = cv11_5\n",
    "\n",
    "model_cv11 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv11_train)\n",
    "result_model_cv11 = model_cv11.fit()\n",
    "print(result_model_cv11.summary())\n",
    "\n",
    "cv11_test['log_units_cv'] = result_model_cv11.predict(cv11_test)\n",
    "cv11_test['units_cv'] = np.exp(cv11_test['log_units_cv']) - 1\n",
    "\n",
    "cv11_test['RMSLE'] = np.log((cv11_test['units_cv'] + 1) / (cv11_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv11_test['RMSLE']) / len(cv11_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_11 = get_item_nbr(df_11) #11번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence11 = result_model11.get_influence()\n",
    "\n",
    "cooks_d2_11, pvals11 = influence11.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr11 = 4 / (len(df_11) - 778)\n",
    "idx11 = np.where(cooks_d2_11 > fox_cr11)[0]\n",
    "\n",
    "print(len(idx11)) #outlier 갯수\n",
    "\n",
    "X11 = result_model11.predict(df_11)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X11, df_11['log_units'])\n",
    "plt.scatter(X11[idx11], df_11.loc[idx11]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx11)), idx11, list(zip(X11[idx11], df_11.loc[idx11]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx11), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx11 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_11 :\n",
    "        if item_nbr == df_11.loc[num].item_nbr :\n",
    "            mean = df_11[df_11['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_11.set_value(num, 'log_units', mean)\n",
    "            df_11.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model11_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_11)\n",
    "result_model11_new = model11_new.fit()\n",
    "print(result_model11_new.summary())\n",
    "\n",
    "test_11_new = processed_test[processed_test['store_nbr'] == 11]\n",
    "\n",
    "test_11_new['log_units'] = result_model11_new.predict(test_11_new)\n",
    "test_11_new['units'] = np.exp(test_11_new['log_units']) - 1\n",
    "result_test2.append(test_11_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_12 = processed_train[processed_train['store_nbr'] == 12].reset_index(drop = True)\n",
    "\n",
    "model12 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_12)\n",
    "result_model12 = model12.fit()\n",
    "print(result_model12.summary())\n",
    "\n",
    "test_12 = processed_test[processed_test['store_nbr'] == 12]\n",
    "\n",
    "test_12['log_units'] = result_model12.predict(test_12)\n",
    "test_12['units'] = np.exp(test_12['log_units']) - 1\n",
    "result_test.append(test_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv12_1 = df_12[:10101]\n",
    "cv12_2 = df_12[10101:20202]\n",
    "cv12_3 = df_12[20202:30414]\n",
    "cv12_4 = df_12[30414:40515]\n",
    "cv12_5 = df_12[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv12_train = pd.concat([cv12_1, cv12_2, cv12_3, cv12_4], ignore_index = True)\n",
    "cv12_test = cv12_5\n",
    "\n",
    "model_cv12 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv12_train)\n",
    "result_model_cv12 = model_cv12.fit()\n",
    "print(result_model_cv12.summary())\n",
    "\n",
    "cv12_test['log_units_cv'] = result_model_cv12.predict(cv12_test)\n",
    "cv12_test['units_cv'] = np.exp(cv12_test['log_units_cv']) - 1\n",
    "\n",
    "cv12_test['RMSLE'] = np.log((cv12_test['units_cv'] + 1) / (cv12_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv12_test['RMSLE']) / len(cv12_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_12 = get_item_nbr(df_12) #12번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence12 = result_model12.get_influence()\n",
    "\n",
    "cooks_d2_12, pvals12 = influence12.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr12 = 4 / (len(df_12) - 778)\n",
    "idx12 = np.where(cooks_d2_12 > fox_cr12)[0]\n",
    "\n",
    "print(len(idx12)) #outlier 갯수\n",
    "\n",
    "X12 = result_model12.predict(df_12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X12, df_12['log_units'])\n",
    "plt.scatter(X12[idx12], df_12.loc[idx12]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx12)), idx12, list(zip(X12[idx12], df_12.loc[idx12]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx12), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx12 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_12 :\n",
    "        if item_nbr == df_12.loc[num].item_nbr :\n",
    "            mean = df_12[df_12['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_12.set_value(num, 'log_units', mean)\n",
    "            df_12.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model12_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_12)\n",
    "result_model12_new = model12_new.fit()\n",
    "print(result_model12_new.summary())\n",
    "\n",
    "test_12_new = processed_test[processed_test['store_nbr'] == 12]\n",
    "\n",
    "test_12_new['log_units'] = result_model12_new.predict(test_12_new)\n",
    "test_12_new['units'] = np.exp(test_12_new['log_units']) - 1\n",
    "result_test2.append(test_12_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_13 = processed_train[processed_train['store_nbr'] == 13].reset_index(drop = True)\n",
    "\n",
    "model13 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_13)\n",
    "result_model13 = model13.fit()\n",
    "print(result_model13.summary())\n",
    "\n",
    "test_13 = processed_test[processed_test['store_nbr'] == 13]\n",
    "\n",
    "test_13['log_units'] = result_model13.predict(test_13)\n",
    "test_13['units'] = np.exp(test_13['log_units']) - 1\n",
    "result_test.append(test_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv13_1 = df_13[:10101]\n",
    "cv13_2 = df_13[10101:20202]\n",
    "cv13_3 = df_13[20202:30414]\n",
    "cv13_4 = df_13[30414:40515]\n",
    "cv13_5 = df_13[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv13_train = pd.concat([cv13_1, cv13_2, cv13_3, cv13_4], ignore_index = True)\n",
    "cv13_test = cv13_5\n",
    "\n",
    "model_cv13 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv13_train)\n",
    "result_model_cv13 = model_cv13.fit()\n",
    "print(result_model_cv13.summary())\n",
    "\n",
    "cv13_test['log_units_cv'] = result_model_cv13.predict(cv13_test)\n",
    "cv13_test['units_cv'] = np.exp(cv13_test['log_units_cv']) - 1\n",
    "\n",
    "cv13_test['RMSLE'] = np.log((cv13_test['units_cv'] + 1) / (cv13_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv13_test['RMSLE']) / len(cv13_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_13 = get_item_nbr(df_13) #13번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence13 = result_model13.get_influence()\n",
    "\n",
    "cooks_d2_13, pvals13 = influence13.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr13 = 4 / (len(df_13) - 778)\n",
    "idx13 = np.where(cooks_d2_13 > fox_cr13)[0]\n",
    "\n",
    "print(len(idx13)) #outlier 갯수\n",
    "\n",
    "X13 = result_model13.predict(df_13)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X13, df_13['log_units'])\n",
    "plt.scatter(X13[idx13], df_13.loc[idx13]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx13)), idx13, list(zip(X13[idx13], df_13.loc[idx13]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx13), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx13 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_13 :\n",
    "        if item_nbr == df_13.loc[num].item_nbr :\n",
    "            mean = df_13[df_13['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_13.set_value(num, 'log_units', mean)\n",
    "            df_13.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model13_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_13)\n",
    "result_model13_new = model13_new.fit()\n",
    "print(result_model13_new.summary())\n",
    "\n",
    "test_13_new = processed_test[processed_test['store_nbr'] == 13]\n",
    "\n",
    "test_13_new['log_units'] = result_model13_new.predict(test_13_new)\n",
    "test_13_new['units'] = np.exp(test_13_new['log_units']) - 1\n",
    "result_test2.append(test_13_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_14 = processed_train[processed_train['store_nbr'] == 14].reset_index(drop = True)\n",
    "\n",
    "model14 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_14)\n",
    "result_model14 = model14.fit()\n",
    "print(result_model14.summary())\n",
    "\n",
    "test_14 = processed_test[processed_test['store_nbr'] == 14]\n",
    "\n",
    "test_14['log_units'] = result_model14.predict(test_14)\n",
    "test_14['units'] = np.exp(test_14['log_units']) - 1\n",
    "result_test.append(test_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv14_1 = df_14[:10101]\n",
    "cv14_2 = df_14[10101:20202]\n",
    "cv14_3 = df_14[20202:30414]\n",
    "cv14_4 = df_14[30414:40515]\n",
    "cv14_5 = df_14[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv14_train = pd.concat([cv14_1, cv14_2, cv14_3, cv14_4], ignore_index = True)\n",
    "cv14_test = cv14_5\n",
    "\n",
    "model_cv14 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv14_train)\n",
    "result_model_cv14 = model_cv14.fit()\n",
    "print(result_model_cv14.summary())\n",
    "\n",
    "cv14_test['log_units_cv'] = result_model_cv14.predict(cv14_test)\n",
    "cv14_test['units_cv'] = np.exp(cv14_test['log_units_cv']) - 1\n",
    "\n",
    "cv14_test['RMSLE'] = np.log((cv14_test['units_cv'] + 1) / (cv14_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv14_test['RMSLE']) / len(cv14_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_14 = get_item_nbr(df_14) #14번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence14 = result_model14.get_influence()\n",
    "\n",
    "cooks_d2_14, pvals14 = influence14.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr14 = 4 / (len(df_14) - 778)\n",
    "idx14 = np.where(cooks_d2_14 > fox_cr14)[0]\n",
    "\n",
    "print(len(idx14)) #outlier 갯수\n",
    "\n",
    "X14 = result_model14.predict(df_14)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X14, df_14['log_units'])\n",
    "plt.scatter(X14[idx14], df_14.loc[idx14]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx14)), idx14, list(zip(X14[idx14], df_14.loc[idx14]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx14), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx14 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_14 :\n",
    "        if item_nbr == df_14.loc[num].item_nbr :\n",
    "            mean = df_14[df_14['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_14.set_value(num, 'log_units', mean)\n",
    "            df_14.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model14_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_14)\n",
    "result_model14_new = model14_new.fit()\n",
    "print(result_model14_new.summary())\n",
    "\n",
    "test_14_new = processed_test[processed_test['store_nbr'] == 14]\n",
    "\n",
    "test_14_new['log_units'] = result_model14_new.predict(test_14_new)\n",
    "test_14_new['units'] = np.exp(test_14_new['log_units']) - 1\n",
    "result_test2.append(test_14_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_15 = processed_train[processed_train['store_nbr'] == 15].reset_index(drop = True)\n",
    "\n",
    "model15 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_15)\n",
    "result_model15 = model15.fit()\n",
    "print(result_model15.summary())\n",
    "\n",
    "test_15 = processed_test[processed_test['store_nbr'] == 15]\n",
    "\n",
    "test_15['log_units'] = result_model15.predict(test_15)\n",
    "test_15['units'] = np.exp(test_15['log_units']) - 1\n",
    "result_test.append(test_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv15_1 = df_15[:10101]\n",
    "cv15_2 = df_15[10101:20202]\n",
    "cv15_3 = df_15[20202:30414]\n",
    "cv15_4 = df_15[30414:40515]\n",
    "cv15_5 = df_15[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv15_train = pd.concat([cv15_1, cv15_2, cv15_3, cv15_4], ignore_index = True)\n",
    "cv15_test = cv15_5\n",
    "\n",
    "model_cv15 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv15_train)\n",
    "result_model_cv15 = model_cv15.fit()\n",
    "print(result_model_cv15.summary())\n",
    "\n",
    "cv15_test['log_units_cv'] = result_model_cv15.predict(cv15_test)\n",
    "cv15_test['units_cv'] = np.exp(cv15_test['log_units_cv']) - 1\n",
    "\n",
    "cv15_test['RMSLE'] = np.log((cv15_test['units_cv'] + 1) / (cv15_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv15_test['RMSLE']) / len(cv15_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_15 = get_item_nbr(df_15) #15번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence15 = result_model15.get_influence()\n",
    "\n",
    "cooks_d2_15, pvals15 = influence15.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr15 = 4 / (len(df_15) - 778)\n",
    "idx15 = np.where(cooks_d2_15 > fox_cr15)[0]\n",
    "\n",
    "print(len(idx15)) #outlier 갯수\n",
    "\n",
    "X15 = result_model15.predict(df_15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X15, df_15['log_units'])\n",
    "plt.scatter(X15[idx15], df_15.loc[idx15]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx15)), idx15, list(zip(X15[idx15], df_15.loc[idx15]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx15), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx15 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_15 :\n",
    "        if item_nbr == df_15.loc[num].item_nbr :\n",
    "            mean = df_15[df_15['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_15.set_value(num, 'log_units', mean)\n",
    "            df_15.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model15_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_15)\n",
    "result_model15_new = model15_new.fit()\n",
    "print(result_model15_new.summary())\n",
    "\n",
    "test_15_new = processed_test[processed_test['store_nbr'] == 15]\n",
    "\n",
    "test_15_new['log_units'] = result_model15_new.predict(test_15_new)\n",
    "test_15_new['units'] = np.exp(test_15_new['log_units']) - 1\n",
    "result_test2.append(test_15_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_16 = processed_train[processed_train['store_nbr'] == 16].reset_index(drop = True)\n",
    "\n",
    "model16 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_16)\n",
    "result_model16 = model16.fit()\n",
    "print(result_model16.summary())\n",
    "\n",
    "test_16 = processed_test[processed_test['store_nbr'] == 16]\n",
    "\n",
    "test_16['log_units'] = result_model16.predict(test_16)\n",
    "test_16['units'] = np.exp(test_16['log_units']) - 1\n",
    "result_test.append(test_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv16_1 = df_16[:10101]\n",
    "cv16_2 = df_16[10101:20202]\n",
    "cv16_3 = df_16[20202:30414]\n",
    "cv16_4 = df_16[30414:40515]\n",
    "cv16_5 = df_16[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv16_train = pd.concat([cv16_1, cv16_2, cv16_3, cv16_4], ignore_index = True)\n",
    "cv16_test = cv16_5\n",
    "\n",
    "model_cv16 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv16_train)\n",
    "result_model_cv16 = model_cv16.fit()\n",
    "print(result_model_cv16.summary())\n",
    "\n",
    "cv16_test['log_units_cv'] = result_model_cv16.predict(cv16_test)\n",
    "cv16_test['units_cv'] = np.exp(cv16_test['log_units_cv']) - 1\n",
    "\n",
    "cv16_test['RMSLE'] = np.log((cv16_test['units_cv'] + 1) / (cv16_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv16_test['RMSLE']) / len(cv16_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_16 = get_item_nbr(df_16) #16번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence16 = result_model16.get_influence()\n",
    "\n",
    "cooks_d2_16, pvals16 = influence16.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr16 = 4 / (len(df_16) - 778)\n",
    "idx16 = np.where(cooks_d2_16 > fox_cr16)[0]\n",
    "\n",
    "print(len(idx16)) #outlier 갯수\n",
    "\n",
    "X16 = result_model16.predict(df_16)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X16, df_16['log_units'])\n",
    "plt.scatter(X16[idx16], df_16.loc[idx16]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx16)), idx16, list(zip(X16[idx16], df_16.loc[idx16]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx16), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx16 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_16 :\n",
    "        if item_nbr == df_16.loc[num].item_nbr :\n",
    "            mean = df_16[df_16['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_16.set_value(num, 'log_units', mean)\n",
    "            df_16.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model16_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_16)\n",
    "result_model16_new = model16_new.fit()\n",
    "print(result_model16_new.summary())\n",
    "\n",
    "test_16_new = processed_test[processed_test['store_nbr'] == 16]\n",
    "\n",
    "test_16_new['log_units'] = result_model16_new.predict(test_16_new)\n",
    "test_16_new['units'] = np.exp(test_16_new['log_units']) - 1\n",
    "result_test2.append(test_16_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_17 = processed_train[processed_train['store_nbr'] == 17].reset_index(drop = True)\n",
    "\n",
    "model17 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_17)\n",
    "result_model17 = model17.fit()\n",
    "print(result_model17.summary())\n",
    "\n",
    "test_17 = processed_test[processed_test['store_nbr'] == 17]\n",
    "\n",
    "test_17['log_units'] = result_model17.predict(test_17)\n",
    "test_17['units'] = np.exp(test_17['log_units']) - 1\n",
    "result_test.append(test_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv17_1 = df_17[:10101]\n",
    "cv17_2 = df_17[10101:20202]\n",
    "cv17_3 = df_17[20202:30414]\n",
    "cv17_4 = df_17[30414:40515]\n",
    "cv17_5 = df_17[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv17_train = pd.concat([cv17_1, cv17_2, cv17_3, cv17_4], ignore_index = True)\n",
    "cv17_test = cv17_5\n",
    "\n",
    "model_cv17 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv17_train)\n",
    "result_model_cv17 = model_cv17.fit()\n",
    "print(result_model_cv17.summary())\n",
    "\n",
    "cv17_test['log_units_cv'] = result_model_cv17.predict(cv17_test)\n",
    "cv17_test['units_cv'] = np.exp(cv17_test['log_units_cv']) - 1\n",
    "\n",
    "cv17_test['RMSLE'] = np.log((cv17_test['units_cv'] + 1) / (cv17_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv17_test['RMSLE']) / len(cv17_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_17 = get_item_nbr(df_17) #17번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence17 = result_model17.get_influence()\n",
    "\n",
    "cooks_d2_17, pvals17 = influence17.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr17 = 4 / (len(df_17) - 778)\n",
    "idx17 = np.where(cooks_d2_17 > fox_cr17)[0]\n",
    "\n",
    "print(len(idx17)) #outlier 갯수\n",
    "\n",
    "X17 = result_model17.predict(df_17)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X17, df_17['log_units'])\n",
    "plt.scatter(X17[idx17], df_17.loc[idx17]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx17)), idx17, list(zip(X17[idx17], df_17.loc[idx17]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx17), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx17 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_17 :\n",
    "        if item_nbr == df_17.loc[num].item_nbr :\n",
    "            mean = df_17[df_17['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_17.set_value(num, 'log_units', mean)\n",
    "            df_17.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model17_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_17)\n",
    "result_model17_new = model17_new.fit()\n",
    "print(result_model17_new.summary())\n",
    "\n",
    "test_17_new = processed_test[processed_test['store_nbr'] == 17]\n",
    "\n",
    "test_17_new['log_units'] = result_model17_new.predict(test_17_new)\n",
    "test_17_new['units'] = np.exp(test_17_new['log_units']) - 1\n",
    "result_test2.append(test_17_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_18 = processed_train[processed_train['store_nbr'] == 18].reset_index(drop = True)\n",
    "\n",
    "model18 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_18)\n",
    "result_model18 = model18.fit()\n",
    "print(result_model18.summary())\n",
    "\n",
    "test_18 = processed_test[processed_test['store_nbr'] == 18]\n",
    "\n",
    "test_18['log_units'] = result_model18.predict(test_18)\n",
    "test_18['units'] = np.exp(test_18['log_units']) - 1\n",
    "result_test.append(test_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv18_1 = df_18[:10101]\n",
    "cv18_2 = df_18[10101:20202]\n",
    "cv18_3 = df_18[20202:30414]\n",
    "cv18_4 = df_18[30414:40515]\n",
    "cv18_5 = df_18[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv18_train = pd.concat([cv18_1, cv18_2, cv18_3, cv18_4], ignore_index = True)\n",
    "cv18_test = cv18_5\n",
    "\n",
    "model_cv18 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv18_train)\n",
    "result_model_cv18 = model_cv18.fit()\n",
    "print(result_model_cv18.summary())\n",
    "\n",
    "cv18_test['log_units_cv'] = result_model_cv18.predict(cv18_test)\n",
    "cv18_test['units_cv'] = np.exp(cv18_test['log_units_cv']) - 1\n",
    "\n",
    "cv18_test['RMSLE'] = np.log((cv18_test['units_cv'] + 1) / (cv18_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv18_test['RMSLE']) / len(cv18_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_18 = get_item_nbr(df_18) #18번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence18 = result_model18.get_influence()\n",
    "\n",
    "cooks_d2_18, pvals18 = influence18.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr18 = 4 / (len(df_18) - 778)\n",
    "idx18 = np.where(cooks_d2_18 > fox_cr18)[0]\n",
    "\n",
    "print(len(idx18)) #outlier 갯수\n",
    "\n",
    "X18 = result_model18.predict(df_18)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X18, df_18['log_units'])\n",
    "plt.scatter(X18[idx18], df_18.loc[idx18]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx18)), idx18, list(zip(X18[idx18], df_18.loc[idx18]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx18), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx18 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_18 :\n",
    "        if item_nbr == df_18.loc[num].item_nbr :\n",
    "            mean = df_18[df_18['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_18.set_value(num, 'log_units', mean)\n",
    "            df_18.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model18_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_18)\n",
    "result_model18_new = model18_new.fit()\n",
    "print(result_model18_new.summary())\n",
    "\n",
    "test_18_new = processed_test[processed_test['store_nbr'] == 18]\n",
    "\n",
    "test_18_new['log_units'] = result_model18_new.predict(test_18_new)\n",
    "test_18_new['units'] = np.exp(test_18_new['log_units']) - 1\n",
    "result_test2.append(test_18_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_19 = processed_train[processed_train['store_nbr'] == 19].reset_index(drop = True)\n",
    "\n",
    "model19 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_19)\n",
    "result_model19 = model19.fit()\n",
    "print(result_model19.summary())\n",
    "\n",
    "test_19 = processed_test[processed_test['store_nbr'] == 19]\n",
    "\n",
    "test_19['log_units'] = result_model19.predict(test_19)\n",
    "test_19['units'] = np.exp(test_19['log_units']) - 1\n",
    "result_test.append(test_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv19_1 = df_19[:10101]\n",
    "cv19_2 = df_19[10101:20202]\n",
    "cv19_3 = df_19[20202:30414]\n",
    "cv19_4 = df_19[30414:40515]\n",
    "cv19_5 = df_19[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv19_train = pd.concat([cv19_1, cv19_2, cv19_3, cv19_4], ignore_index = True)\n",
    "cv19_test = cv19_5\n",
    "\n",
    "model_cv19 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv19_train)\n",
    "result_model_cv19 = model_cv19.fit()\n",
    "print(result_model_cv19.summary())\n",
    "\n",
    "cv19_test['log_units_cv'] = result_model_cv19.predict(cv19_test)\n",
    "cv19_test['units_cv'] = np.exp(cv19_test['log_units_cv']) - 1\n",
    "\n",
    "cv19_test['RMSLE'] = np.log((cv19_test['units_cv'] + 1) / (cv19_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv19_test['RMSLE']) / len(cv19_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_19 = get_item_nbr(df_19) #19번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence19 = result_model19.get_influence()\n",
    "\n",
    "cooks_d2_19, pvals19 = influence19.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr19 = 4 / (len(df_19) - 778)\n",
    "idx19 = np.where(cooks_d2_19 > fox_cr19)[0]\n",
    "\n",
    "print(len(idx19)) #outlier 갯수\n",
    "\n",
    "X19 = result_model19.predict(df_19)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X19, df_19['log_units'])\n",
    "plt.scatter(X19[idx19], df_19.loc[idx19]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx19)), idx19, list(zip(X19[idx19], df_19.loc[idx19]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx19), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx19 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_19 :\n",
    "        if item_nbr == df_19.loc[num].item_nbr :\n",
    "            mean = df_19[df_19['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_19.set_value(num, 'log_units', mean)\n",
    "            df_19.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model19_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_19)\n",
    "result_model19_new = model19_new.fit()\n",
    "print(result_model19_new.summary())\n",
    "\n",
    "test_19_new = processed_test[processed_test['store_nbr'] == 19]\n",
    "\n",
    "test_19_new['log_units'] = result_model19_new.predict(test_19_new)\n",
    "test_19_new['units'] = np.exp(test_19_new['log_units']) - 1\n",
    "result_test2.append(test_19_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_20 = processed_train[processed_train['store_nbr'] == 20].reset_index(drop = True)\n",
    "\n",
    "model20 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_20)\n",
    "result_model20 = model20.fit()\n",
    "print(result_model20.summary())\n",
    "\n",
    "test_20 = processed_test[processed_test['store_nbr'] == 20]\n",
    "\n",
    "test_20['log_units'] = result_model20.predict(test_20)\n",
    "test_20['units'] = np.exp(test_20['log_units']) - 1\n",
    "result_test.append(test_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv20_1 = df_20[:10101]\n",
    "cv20_2 = df_20[10101:20202]\n",
    "cv20_3 = df_20[20202:30414]\n",
    "cv20_4 = df_20[30414:40515]\n",
    "cv20_5 = df_20[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv20_train = pd.concat([cv20_1, cv20_2, cv20_3, cv20_4], ignore_index = True)\n",
    "cv20_test = cv20_5\n",
    "\n",
    "model_cv20 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv20_train)\n",
    "result_model_cv20 = model_cv20.fit()\n",
    "print(result_model_cv20.summary())\n",
    "\n",
    "cv20_test['log_units_cv'] = result_model_cv20.predict(cv20_test)\n",
    "cv20_test['units_cv'] = np.exp(cv20_test['log_units_cv']) - 1\n",
    "\n",
    "cv20_test['RMSLE'] = np.log((cv20_test['units_cv'] + 1) / (cv20_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv20_test['RMSLE']) / len(cv20_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_20 = get_item_nbr(df_20) #20번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence20 = result_model20.get_influence()\n",
    "\n",
    "cooks_d2_20, pvals20 = influence20.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr20 = 4 / (len(df_20) - 778)\n",
    "idx20 = np.where(cooks_d2_20 > fox_cr20)[0]\n",
    "\n",
    "print(len(idx20)) #outlier 갯수\n",
    "\n",
    "X20 = result_model20.predict(df_20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X20, df_20['log_units'])\n",
    "plt.scatter(X20[idx20], df_20.loc[idx20]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx20)), idx20, list(zip(X20[idx20], df_20.loc[idx20]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx20), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx20 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_20 :\n",
    "        if item_nbr == df_20.loc[num].item_nbr :\n",
    "            mean = df_20[df_20['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_20.set_value(num, 'log_units', mean)\n",
    "            df_20.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model20_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_20)\n",
    "result_model20_new = model20_new.fit()\n",
    "print(result_model20_new.summary())\n",
    "\n",
    "test_20_new = processed_test[processed_test['store_nbr'] == 20]\n",
    "\n",
    "test_20_new['log_units'] = result_model20_new.predict(test_20_new)\n",
    "test_20_new['units'] = np.exp(test_20_new['log_units']) - 1\n",
    "result_test2.append(test_20_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_21 = processed_train[processed_train['store_nbr'] == 21].reset_index(drop = True)\n",
    "\n",
    "model21 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_21)\n",
    "result_model21 = model21.fit()\n",
    "print(result_model21.summary())\n",
    "\n",
    "test_21 = processed_test[processed_test['store_nbr'] == 21]\n",
    "\n",
    "test_21['log_units'] = result_model21.predict(test_21)\n",
    "test_21['units'] = np.exp(test_21['log_units']) - 1\n",
    "result_test.append(test_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv21_1 = df_21[:10101]\n",
    "cv21_2 = df_21[10101:20202]\n",
    "cv21_3 = df_21[20202:30414]\n",
    "cv21_4 = df_21[30414:40515]\n",
    "cv21_5 = df_21[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv21_train = pd.concat([cv21_1, cv21_2, cv21_3, cv21_4], ignore_index = True)\n",
    "cv21_test = cv21_5\n",
    "\n",
    "model_cv21 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv21_train)\n",
    "result_model_cv21 = model_cv21.fit()\n",
    "print(result_model_cv21.summary())\n",
    "\n",
    "cv21_test['log_units_cv'] = result_model_cv21.predict(cv21_test)\n",
    "cv21_test['units_cv'] = np.exp(cv21_test['log_units_cv']) - 1\n",
    "\n",
    "cv21_test['RMSLE'] = np.log((cv21_test['units_cv'] + 1) / (cv21_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv21_test['RMSLE']) / len(cv21_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_21 = get_item_nbr(df_21) #21번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence21 = result_model21.get_influence()\n",
    "\n",
    "cooks_d2_21, pvals21 = influence21.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr21 = 4 / (len(df_21) - 778)\n",
    "idx21 = np.where(cooks_d2_21 > fox_cr21)[0]\n",
    "\n",
    "print(len(idx21)) #outlier 갯수\n",
    "\n",
    "X21 = result_model21.predict(df_21)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X21, df_21['log_units'])\n",
    "plt.scatter(X21[idx21], df_21.loc[idx21]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx21)), idx21, list(zip(X21[idx21], df_21.loc[idx21]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx21), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx21 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_21 :\n",
    "        if item_nbr == df_21.loc[num].item_nbr :\n",
    "            mean = df_21[df_21['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_21.set_value(num, 'log_units', mean)\n",
    "            df_21.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model21_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_21)\n",
    "result_model21_new = model21_new.fit()\n",
    "print(result_model21_new.summary())\n",
    "\n",
    "test_21_new = processed_test[processed_test['store_nbr'] == 21]\n",
    "\n",
    "test_21_new['log_units'] = result_model21_new.predict(test_21_new)\n",
    "test_21_new['units'] = np.exp(test_21_new['log_units']) - 1\n",
    "result_test2.append(test_21_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_22 = processed_train[processed_train['store_nbr'] == 22].reset_index(drop = True)\n",
    "\n",
    "model22 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_22)\n",
    "result_model22 = model22.fit()\n",
    "print(result_model22.summary())\n",
    "\n",
    "test_22 = processed_test[processed_test['store_nbr'] == 22]\n",
    "\n",
    "test_22['log_units'] = result_model22.predict(test_22)\n",
    "test_22['units'] = np.exp(test_22['log_units']) - 1\n",
    "result_test.append(test_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv22_1 = df_22[:10101]\n",
    "cv22_2 = df_22[10101:20202]\n",
    "cv22_3 = df_22[20202:30414]\n",
    "cv22_4 = df_22[30414:40515]\n",
    "cv22_5 = df_22[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv22_train = pd.concat([cv22_1, cv22_2, cv22_3, cv22_4], ignore_index = True)\n",
    "cv22_test = cv22_5\n",
    "\n",
    "model_cv22 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv22_train)\n",
    "result_model_cv22 = model_cv22.fit()\n",
    "print(result_model_cv22.summary())\n",
    "\n",
    "cv22_test['log_units_cv'] = result_model_cv22.predict(cv22_test)\n",
    "cv22_test['units_cv'] = np.exp(cv22_test['log_units_cv']) - 1\n",
    "\n",
    "cv22_test['RMSLE'] = np.log((cv22_test['units_cv'] + 1) / (cv22_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv22_test['RMSLE']) / len(cv22_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_22 = get_item_nbr(df_22) #22번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence22 = result_model22.get_influence()\n",
    "\n",
    "cooks_d2_22, pvals22 = influence22.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr22 = 4 / (len(df_22) - 778)\n",
    "idx22 = np.where(cooks_d2_22 > fox_cr22)[0]\n",
    "\n",
    "print(len(idx22)) #outlier 갯수\n",
    "\n",
    "X22 = result_model22.predict(df_22)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X22, df_22['log_units'])\n",
    "plt.scatter(X22[idx22], df_22.loc[idx22]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx22)), idx22, list(zip(X22[idx22], df_22.loc[idx22]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx22), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx22 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_22 :\n",
    "        if item_nbr == df_22.loc[num].item_nbr :\n",
    "            mean = df_22[df_22['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_22.set_value(num, 'log_units', mean)\n",
    "            df_22.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model22_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_22)\n",
    "result_model22_new = model22_new.fit()\n",
    "print(result_model22_new.summary())\n",
    "\n",
    "test_22_new = processed_test[processed_test['store_nbr'] == 22]\n",
    "\n",
    "test_22_new['log_units'] = result_model22_new.predict(test_22_new)\n",
    "test_22_new['units'] = np.exp(test_22_new['log_units']) - 1\n",
    "result_test2.append(test_22_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_23 = processed_train[processed_train['store_nbr'] == 23].reset_index(drop = True)\n",
    "\n",
    "model23 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_23)\n",
    "result_model23 = model23.fit()\n",
    "print(result_model23.summary())\n",
    "\n",
    "test_23 = processed_test[processed_test['store_nbr'] == 23]\n",
    "\n",
    "test_23['log_units'] = result_model23.predict(test_23)\n",
    "test_23['units'] = np.exp(test_23['log_units']) - 1\n",
    "result_test.append(test_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv23_1 = df_23[:10101]\n",
    "cv23_2 = df_23[10101:20202]\n",
    "cv23_3 = df_23[20202:30414]\n",
    "cv23_4 = df_23[30414:40515]\n",
    "cv23_5 = df_23[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv23_train = pd.concat([cv23_1, cv23_2, cv23_3, cv23_4], ignore_index = True)\n",
    "cv23_test = cv23_5\n",
    "\n",
    "model_cv23 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv23_train)\n",
    "result_model_cv23 = model_cv23.fit()\n",
    "print(result_model_cv23.summary())\n",
    "\n",
    "cv23_test['log_units_cv'] = result_model_cv23.predict(cv23_test)\n",
    "cv23_test['units_cv'] = np.exp(cv23_test['log_units_cv']) - 1\n",
    "\n",
    "cv23_test['RMSLE'] = np.log((cv23_test['units_cv'] + 1) / (cv23_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv23_test['RMSLE']) / len(cv23_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_23 = get_item_nbr(df_23) #23번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence23 = result_model23.get_influence()\n",
    "\n",
    "cooks_d2_23, pvals23 = influence23.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr23 = 4 / (len(df_23) - 778)\n",
    "idx23 = np.where(cooks_d2_23 > fox_cr23)[0]\n",
    "\n",
    "print(len(idx23)) #outlier 갯수\n",
    "\n",
    "X23 = result_model23.predict(df_23)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X23, df_23['log_units'])\n",
    "plt.scatter(X23[idx23], df_23.loc[idx23]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx23)), idx23, list(zip(X23[idx23], df_23.loc[idx23]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx23), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx23 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_23 :\n",
    "        if item_nbr == df_23.loc[num].item_nbr :\n",
    "            mean = df_23[df_23['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_23.set_value(num, 'log_units', mean)\n",
    "            df_23.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model23_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_23)\n",
    "result_model23_new = model23_new.fit()\n",
    "print(result_model23_new.summary())\n",
    "\n",
    "test_23_new = processed_test[processed_test['store_nbr'] == 23]\n",
    "\n",
    "test_23_new['log_units'] = result_model23_new.predict(test_23_new)\n",
    "test_23_new['units'] = np.exp(test_23_new['log_units']) - 1\n",
    "result_test2.append(test_23_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_24 = processed_train[processed_train['store_nbr'] == 24].reset_index(drop = True)\n",
    "\n",
    "model24 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_24)\n",
    "result_model24 = model24.fit()\n",
    "print(result_model24.summary())\n",
    "\n",
    "test_24 = processed_test[processed_test['store_nbr'] == 24]\n",
    "\n",
    "test_24['log_units'] = result_model24.predict(test_24)\n",
    "test_24['units'] = np.exp(test_24['log_units']) - 1\n",
    "result_test.append(test_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv24_1 = df_24[:10101]\n",
    "cv24_2 = df_24[10101:20202]\n",
    "cv24_3 = df_24[20202:30414]\n",
    "cv24_4 = df_24[30414:40515]\n",
    "cv24_5 = df_24[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv24_train = pd.concat([cv24_1, cv24_2, cv24_3, cv24_4], ignore_index = True)\n",
    "cv24_test = cv24_5\n",
    "\n",
    "model_cv24 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv24_train)\n",
    "result_model_cv24 = model_cv24.fit()\n",
    "print(result_model_cv24.summary())\n",
    "\n",
    "cv24_test['log_units_cv'] = result_model_cv24.predict(cv24_test)\n",
    "cv24_test['units_cv'] = np.exp(cv24_test['log_units_cv']) - 1\n",
    "\n",
    "cv24_test['RMSLE'] = np.log((cv24_test['units_cv'] + 1) / (cv24_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv24_test['RMSLE']) / len(cv24_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_24 = get_item_nbr(df_24) #24번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence24 = result_model24.get_influence()\n",
    "\n",
    "cooks_d2_24, pvals24 = influence24.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr24 = 4 / (len(df_24) - 778)\n",
    "idx24 = np.where(cooks_d2_24 > fox_cr24)[0]\n",
    "\n",
    "print(len(idx24)) #outlier 갯수\n",
    "\n",
    "X24 = result_model24.predict(df_24)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X24, df_24['log_units'])\n",
    "plt.scatter(X24[idx24], df_24.loc[idx24]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx24)), idx24, list(zip(X24[idx24], df_24.loc[idx24]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx24), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx24 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_24 :\n",
    "        if item_nbr == df_24.loc[num].item_nbr :\n",
    "            mean = df_24[df_24['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_24.set_value(num, 'log_units', mean)\n",
    "            df_24.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model24_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_24)\n",
    "result_model24_new = model24_new.fit()\n",
    "print(result_model24_new.summary())\n",
    "\n",
    "test_24_new = processed_test[processed_test['store_nbr'] == 24]\n",
    "\n",
    "test_24_new['log_units'] = result_model24_new.predict(test_24_new)\n",
    "test_24_new['units'] = np.exp(test_24_new['log_units']) - 1\n",
    "result_test2.append(test_24_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_25 = processed_train[processed_train['store_nbr'] == 25].reset_index(drop = True)\n",
    "\n",
    "model25 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_25)\n",
    "result_model25 = model25.fit()\n",
    "print(result_model25.summary())\n",
    "\n",
    "test_25 = processed_test[processed_test['store_nbr'] == 25]\n",
    "\n",
    "test_25['log_units'] = result_model25.predict(test_25)\n",
    "test_25['units'] = np.exp(test_25['log_units']) - 1\n",
    "result_test.append(test_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv25_1 = df_25[:10101]\n",
    "cv25_2 = df_25[10101:20202]\n",
    "cv25_3 = df_25[20202:30414]\n",
    "cv25_4 = df_25[30414:40515]\n",
    "cv25_5 = df_25[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv25_train = pd.concat([cv25_1, cv25_2, cv25_3, cv25_4], ignore_index = True)\n",
    "cv25_test = cv25_5\n",
    "\n",
    "model_cv25 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv25_train)\n",
    "result_model_cv25 = model_cv25.fit()\n",
    "print(result_model_cv25.summary())\n",
    "\n",
    "cv25_test['log_units_cv'] = result_model_cv25.predict(cv25_test)\n",
    "cv25_test['units_cv'] = np.exp(cv25_test['log_units_cv']) - 1\n",
    "\n",
    "cv25_test['RMSLE'] = np.log((cv25_test['units_cv'] + 1) / (cv25_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv25_test['RMSLE']) / len(cv25_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_25 = get_item_nbr(df_25) #25번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence25 = result_model25.get_influence()\n",
    "\n",
    "cooks_d2_25, pvals25 = influence25.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr25 = 4 / (len(df_25) - 778)\n",
    "idx25 = np.where(cooks_d2_25 > fox_cr25)[0]\n",
    "\n",
    "print(len(idx25)) #outlier 갯수\n",
    "\n",
    "X25 = result_model25.predict(df_25)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X25, df_25['log_units'])\n",
    "plt.scatter(X25[idx25], df_25.loc[idx25]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx25)), idx25, list(zip(X25[idx25], df_25.loc[idx25]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx25), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx25 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_25 :\n",
    "        if item_nbr == df_25.loc[num].item_nbr :\n",
    "            mean = df_25[df_25['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_25.set_value(num, 'log_units', mean)\n",
    "            df_25.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model25_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_25)\n",
    "result_model25_new = model25_new.fit()\n",
    "print(result_model25_new.summary())\n",
    "\n",
    "test_25_new = processed_test[processed_test['store_nbr'] == 25]\n",
    "\n",
    "test_25_new['log_units'] = result_model25_new.predict(test_25_new)\n",
    "test_25_new['units'] = np.exp(test_25_new['log_units']) - 1\n",
    "result_test2.append(test_25_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_26 = processed_train[processed_train['store_nbr'] == 26].reset_index(drop = True)\n",
    "\n",
    "model26 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_26)\n",
    "result_model26 = model26.fit()\n",
    "print(result_model26.summary())\n",
    "\n",
    "test_26 = processed_test[processed_test['store_nbr'] == 26]\n",
    "\n",
    "test_26['log_units'] = result_model26.predict(test_26)\n",
    "test_26['units'] = np.exp(test_26['log_units']) - 1\n",
    "result_test.append(test_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv26_1 = df_26[:10101]\n",
    "cv26_2 = df_26[10101:20202]\n",
    "cv26_3 = df_26[20202:30414]\n",
    "cv26_4 = df_26[30414:40515]\n",
    "cv26_5 = df_26[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv26_train = pd.concat([cv26_1, cv26_2, cv26_3, cv26_4], ignore_index = True)\n",
    "cv26_test = cv26_5\n",
    "\n",
    "model_cv26 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv26_train)\n",
    "result_model_cv26 = model_cv26.fit()\n",
    "print(result_model_cv26.summary())\n",
    "\n",
    "cv26_test['log_units_cv'] = result_model_cv26.predict(cv26_test)\n",
    "cv26_test['units_cv'] = np.exp(cv26_test['log_units_cv']) - 1\n",
    "\n",
    "cv26_test['RMSLE'] = np.log((cv26_test['units_cv'] + 1) / (cv26_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv26_test['RMSLE']) / len(cv26_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_26 = get_item_nbr(df_26) #26번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence26 = result_model26.get_influence()\n",
    "\n",
    "cooks_d2_26, pvals26 = influence26.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr26 = 4 / (len(df_26) - 778)\n",
    "idx26 = np.where(cooks_d2_26 > fox_cr26)[0]\n",
    "\n",
    "print(len(idx26)) #outlier 갯수\n",
    "\n",
    "X26 = result_model26.predict(df_26)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X26, df_26['log_units'])\n",
    "plt.scatter(X26[idx26], df_26.loc[idx26]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx26)), idx26, list(zip(X26[idx26], df_26.loc[idx26]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx26), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx26 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_26 :\n",
    "        if item_nbr == df_26.loc[num].item_nbr :\n",
    "            mean = df_26[df_26['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_26.set_value(num, 'log_units', mean)\n",
    "            df_26.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model26_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_26)\n",
    "result_model26_new = model26_new.fit()\n",
    "print(result_model26_new.summary())\n",
    "\n",
    "test_26_new = processed_test[processed_test['store_nbr'] == 26]\n",
    "\n",
    "test_26_new['log_units'] = result_model26_new.predict(test_26_new)\n",
    "test_26_new['units'] = np.exp(test_26_new['log_units']) - 1\n",
    "result_test2.append(test_26_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_27 = processed_train[processed_train['store_nbr'] == 27].reset_index(drop = True)\n",
    "\n",
    "model27 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_27)\n",
    "result_model27 = model27.fit()\n",
    "print(result_model27.summary())\n",
    "\n",
    "test_27 = processed_test[processed_test['store_nbr'] == 27]\n",
    "\n",
    "test_27['log_units'] = result_model27.predict(test_27)\n",
    "test_27['units'] = np.exp(test_27['log_units']) - 1\n",
    "result_test.append(test_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv27_1 = df_27[:10101]\n",
    "cv27_2 = df_27[10101:20202]\n",
    "cv27_3 = df_27[20202:30414]\n",
    "cv27_4 = df_27[30414:40515]\n",
    "cv27_5 = df_27[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv27_train = pd.concat([cv27_1, cv27_2, cv27_3, cv27_4], ignore_index = True)\n",
    "cv27_test = cv27_5\n",
    "\n",
    "model_cv27 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv27_train)\n",
    "result_model_cv27 = model_cv27.fit()\n",
    "print(result_model_cv27.summary())\n",
    "\n",
    "cv27_test['log_units_cv'] = result_model_cv27.predict(cv27_test)\n",
    "cv27_test['units_cv'] = np.exp(cv27_test['log_units_cv']) - 1\n",
    "\n",
    "cv27_test['RMSLE'] = np.log((cv27_test['units_cv'] + 1) / (cv27_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv27_test['RMSLE']) / len(cv27_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_27 = get_item_nbr(df_27) #27번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence27 = result_model27.get_influence()\n",
    "\n",
    "cooks_d2_27, pvals27 = influence27.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr27 = 4 / (len(df_27) - 778)\n",
    "idx27 = np.where(cooks_d2_27 > fox_cr27)[0]\n",
    "\n",
    "print(len(idx27)) #outlier 갯수\n",
    "\n",
    "X27 = result_model27.predict(df_27)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X27, df_27['log_units'])\n",
    "plt.scatter(X27[idx27], df_27.loc[idx27]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx27)), idx27, list(zip(X27[idx27], df_27.loc[idx27]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx27), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx27 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_27 :\n",
    "        if item_nbr == df_27.loc[num].item_nbr :\n",
    "            mean = df_27[df_27['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_27.set_value(num, 'log_units', mean)\n",
    "            df_27.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model27_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_27)\n",
    "result_model27_new = model27_new.fit()\n",
    "print(result_model27_new.summary())\n",
    "\n",
    "test_27_new = processed_test[processed_test['store_nbr'] == 27]\n",
    "\n",
    "test_27_new['log_units'] = result_model27_new.predict(test_27_new)\n",
    "test_27_new['units'] = np.exp(test_27_new['log_units']) - 1\n",
    "result_test2.append(test_27_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_28 = processed_train[processed_train['store_nbr'] == 28].reset_index(drop = True)\n",
    "\n",
    "model28 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_28)\n",
    "result_model28 = model28.fit()\n",
    "print(result_model28.summary())\n",
    "\n",
    "test_28 = processed_test[processed_test['store_nbr'] == 28]\n",
    "\n",
    "test_28['log_units'] = result_model28.predict(test_28)\n",
    "test_28['units'] = np.exp(test_28['log_units']) - 1\n",
    "result_test.append(test_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv28_1 = df_28[:10101]\n",
    "cv28_2 = df_28[10101:20202]\n",
    "cv28_3 = df_28[20202:30414]\n",
    "cv28_4 = df_28[30414:40515]\n",
    "cv28_5 = df_28[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv28_train = pd.concat([cv28_1, cv28_2, cv28_3, cv28_4], ignore_index = True)\n",
    "cv28_test = cv28_5\n",
    "\n",
    "model_cv28 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv28_train)\n",
    "result_model_cv28 = model_cv28.fit()\n",
    "print(result_model_cv28.summary())\n",
    "\n",
    "cv28_test['log_units_cv'] = result_model_cv28.predict(cv28_test)\n",
    "cv28_test['units_cv'] = np.exp(cv28_test['log_units_cv']) - 1\n",
    "\n",
    "cv28_test['RMSLE'] = np.log((cv28_test['units_cv'] + 1) / (cv28_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv28_test['RMSLE']) / len(cv28_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_28 = get_item_nbr(df_28) #28번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence28 = result_model28.get_influence()\n",
    "\n",
    "cooks_d2_28, pvals28 = influence28.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr28 = 4 / (len(df_28) - 778)\n",
    "idx28 = np.where(cooks_d2_28 > fox_cr28)[0]\n",
    "\n",
    "print(len(idx28)) #outlier 갯수\n",
    "\n",
    "X28 = result_model28.predict(df_28)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X28, df_28['log_units'])\n",
    "plt.scatter(X28[idx28], df_28.loc[idx28]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx28)), idx28, list(zip(X28[idx28], df_28.loc[idx28]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx28), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx28 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_28 :\n",
    "        if item_nbr == df_28.loc[num].item_nbr :\n",
    "            mean = df_28[df_28['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_28.set_value(num, 'log_units', mean)\n",
    "            df_28.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model28_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_28)\n",
    "result_model28_new = model28_new.fit()\n",
    "print(result_model28_new.summary())\n",
    "\n",
    "test_28_new = processed_test[processed_test['store_nbr'] == 28]\n",
    "\n",
    "test_28_new['log_units'] = result_model28_new.predict(test_28_new)\n",
    "test_28_new['units'] = np.exp(test_28_new['log_units']) - 1\n",
    "result_test2.append(test_28_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_29 = processed_train[processed_train['store_nbr'] == 29].reset_index(drop = True)\n",
    "\n",
    "model29 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_29)\n",
    "result_model29 = model29.fit()\n",
    "print(result_model29.summary())\n",
    "\n",
    "test_29 = processed_test[processed_test['store_nbr'] == 29]\n",
    "\n",
    "test_29['log_units'] = result_model29.predict(test_29)\n",
    "test_29['units'] = np.exp(test_29['log_units']) - 1\n",
    "result_test.append(test_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv29_1 = df_29[:10101]\n",
    "cv29_2 = df_29[10101:20202]\n",
    "cv29_3 = df_29[20202:30414]\n",
    "cv29_4 = df_29[30414:40515]\n",
    "cv29_5 = df_29[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv29_train = pd.concat([cv29_1, cv29_2, cv29_3, cv29_4], ignore_index = True)\n",
    "cv29_test = cv29_5\n",
    "\n",
    "model_cv29 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv29_train)\n",
    "result_model_cv29 = model_cv29.fit()\n",
    "print(result_model_cv29.summary())\n",
    "\n",
    "cv29_test['log_units_cv'] = result_model_cv29.predict(cv29_test)\n",
    "cv29_test['units_cv'] = np.exp(cv29_test['log_units_cv']) - 1\n",
    "\n",
    "cv29_test['RMSLE'] = np.log((cv29_test['units_cv'] + 1) / (cv29_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv29_test['RMSLE']) / len(cv29_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_29 = get_item_nbr(df_29) #29번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence29 = result_model29.get_influence()\n",
    "\n",
    "cooks_d2_29, pvals29 = influence29.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr29 = 4 / (len(df_29) - 778)\n",
    "idx29 = np.where(cooks_d2_29 > fox_cr29)[0]\n",
    "\n",
    "print(len(idx29)) #outlier 갯수\n",
    "\n",
    "X29 = result_model29.predict(df_29)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X29, df_29['log_units'])\n",
    "plt.scatter(X29[idx29], df_29.loc[idx29]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx29)), idx29, list(zip(X29[idx29], df_29.loc[idx29]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx29), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx29 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_29 :\n",
    "        if item_nbr == df_29.loc[num].item_nbr :\n",
    "            mean = df_29[df_29['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_29.set_value(num, 'log_units', mean)\n",
    "            df_29.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model29_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_29)\n",
    "result_model29_new = model29_new.fit()\n",
    "print(result_model29_new.summary())\n",
    "\n",
    "test_29_new = processed_test[processed_test['store_nbr'] == 29]\n",
    "\n",
    "test_29_new['log_units'] = result_model29_new.predict(test_29_new)\n",
    "test_29_new['units'] = np.exp(test_29_new['log_units']) - 1\n",
    "result_test2.append(test_29_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_30 = processed_train[processed_train['store_nbr'] == 30].reset_index(drop = True)\n",
    "\n",
    "model30 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_30)\n",
    "result_model30 = model30.fit()\n",
    "print(result_model30.summary())\n",
    "\n",
    "test_30 = processed_test[processed_test['store_nbr'] == 30]\n",
    "\n",
    "test_30['log_units'] = result_model30.predict(test_30)\n",
    "test_30['units'] = np.exp(test_30['log_units']) - 1\n",
    "result_test.append(test_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv30_1 = df_30[:10101]\n",
    "cv30_2 = df_30[10101:20202]\n",
    "cv30_3 = df_30[20202:30414]\n",
    "cv30_4 = df_30[30414:40515]\n",
    "cv30_5 = df_30[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv30_train = pd.concat([cv30_1, cv30_2, cv30_3, cv30_4], ignore_index = True)\n",
    "cv30_test = cv30_5\n",
    "\n",
    "model_cv30 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv30_train)\n",
    "result_model_cv30 = model_cv30.fit()\n",
    "print(result_model_cv30.summary())\n",
    "\n",
    "cv30_test['log_units_cv'] = result_model_cv30.predict(cv30_test)\n",
    "cv30_test['units_cv'] = np.exp(cv30_test['log_units_cv']) - 1\n",
    "\n",
    "cv30_test['RMSLE'] = np.log((cv30_test['units_cv'] + 1) / (cv30_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv30_test['RMSLE']) / len(cv30_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_30 = get_item_nbr(df_30) #30번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence30 = result_model30.get_influence()\n",
    "\n",
    "cooks_d2_30, pvals30 = influence30.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr30 = 4 / (len(df_30) - 778)\n",
    "idx30 = np.where(cooks_d2_30 > fox_cr30)[0]\n",
    "\n",
    "print(len(idx30)) #outlier 갯수\n",
    "\n",
    "X30 = result_model30.predict(df_30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X30, df_30['log_units'])\n",
    "plt.scatter(X30[idx30], df_30.loc[idx30]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx30)), idx30, list(zip(X30[idx30], df_30.loc[idx30]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx30), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx30 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_30 :\n",
    "        if item_nbr == df_30.loc[num].item_nbr :\n",
    "            mean = df_30[df_30['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_30.set_value(num, 'log_units', mean)\n",
    "            df_30.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model30_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_30)\n",
    "result_model30_new = model30_new.fit()\n",
    "print(result_model30_new.summary())\n",
    "\n",
    "test_30_new = processed_test[processed_test['store_nbr'] == 30]\n",
    "\n",
    "test_30_new['log_units'] = result_model30_new.predict(test_30_new)\n",
    "test_30_new['units'] = np.exp(test_30_new['log_units']) - 1\n",
    "result_test2.append(test_30_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_31 = processed_train[processed_train['store_nbr'] == 31].reset_index(drop = True)\n",
    "\n",
    "model31 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_31)\n",
    "result_model31 = model31.fit()\n",
    "print(result_model31.summary())\n",
    "\n",
    "test_31 = processed_test[processed_test['store_nbr'] == 31]\n",
    "\n",
    "test_31['log_units'] = result_model31.predict(test_31)\n",
    "test_31['units'] = np.exp(test_31['log_units']) - 1\n",
    "result_test.append(test_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv31_1 = df_31[:10101]\n",
    "cv31_2 = df_31[10101:20202]\n",
    "cv31_3 = df_31[20202:30414]\n",
    "cv31_4 = df_31[30414:40515]\n",
    "cv31_5 = df_31[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv31_train = pd.concat([cv31_1, cv31_2, cv31_3, cv31_4], ignore_index = True)\n",
    "cv31_test = cv31_5\n",
    "\n",
    "model_cv31 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv31_train)\n",
    "result_model_cv31 = model_cv31.fit()\n",
    "print(result_model_cv31.summary())\n",
    "\n",
    "cv31_test['log_units_cv'] = result_model_cv31.predict(cv31_test)\n",
    "cv31_test['units_cv'] = np.exp(cv31_test['log_units_cv']) - 1\n",
    "\n",
    "cv31_test['RMSLE'] = np.log((cv31_test['units_cv'] + 1) / (cv31_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv31_test['RMSLE']) / len(cv31_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_31 = get_item_nbr(df_31) #31번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence31 = result_model31.get_influence()\n",
    "\n",
    "cooks_d2_31, pvals31 = influence31.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr31 = 4 / (len(df_31) - 778)\n",
    "idx31 = np.where(cooks_d2_31 > fox_cr31)[0]\n",
    "\n",
    "print(len(idx31)) #outlier 갯수\n",
    "\n",
    "X31 = result_model31.predict(df_31)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X31, df_31['log_units'])\n",
    "plt.scatter(X31[idx31], df_31.loc[idx31]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx31)), idx31, list(zip(X31[idx31], df_31.loc[idx31]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx31), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx31 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_31 :\n",
    "        if item_nbr == df_31.loc[num].item_nbr :\n",
    "            mean = df_31[df_31['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_31.set_value(num, 'log_units', mean)\n",
    "            df_31.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model31_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_31)\n",
    "result_model31_new = model31_new.fit()\n",
    "print(result_model31_new.summary())\n",
    "\n",
    "test_31_new = processed_test[processed_test['store_nbr'] == 31]\n",
    "\n",
    "test_31_new['log_units'] = result_model31_new.predict(test_31_new)\n",
    "test_31_new['units'] = np.exp(test_31_new['log_units']) - 1\n",
    "result_test2.append(test_31_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_32 = processed_train[processed_train['store_nbr'] == 32].reset_index(drop = True)\n",
    "\n",
    "model32 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_32)\n",
    "result_model32 = model32.fit()\n",
    "print(result_model32.summary())\n",
    "\n",
    "test_32 = processed_test[processed_test['store_nbr'] == 32]\n",
    "\n",
    "test_32['log_units'] = result_model32.predict(test_32)\n",
    "test_32['units'] = np.exp(test_32['log_units']) - 1\n",
    "result_test.append(test_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv32_1 = df_32[:10101]\n",
    "cv32_2 = df_32[10101:20202]\n",
    "cv32_3 = df_32[20202:30414]\n",
    "cv32_4 = df_32[30414:40515]\n",
    "cv32_5 = df_32[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv32_train = pd.concat([cv32_1, cv32_2, cv32_3, cv32_4], ignore_index = True)\n",
    "cv32_test = cv32_5\n",
    "\n",
    "model_cv32 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv32_train)\n",
    "result_model_cv32 = model_cv32.fit()\n",
    "print(result_model_cv32.summary())\n",
    "\n",
    "cv32_test['log_units_cv'] = result_model_cv32.predict(cv32_test)\n",
    "cv32_test['units_cv'] = np.exp(cv32_test['log_units_cv']) - 1\n",
    "\n",
    "cv32_test['RMSLE'] = np.log((cv32_test['units_cv'] + 1) / (cv32_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv32_test['RMSLE']) / len(cv32_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_32 = get_item_nbr(df_32) #32번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence32 = result_model32.get_influence()\n",
    "\n",
    "cooks_d2_32, pvals32 = influence32.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr32 = 4 / (len(df_32) - 778)\n",
    "idx32 = np.where(cooks_d2_32 > fox_cr32)[0]\n",
    "\n",
    "print(len(idx32)) #outlier 갯수\n",
    "\n",
    "X32 = result_model32.predict(df_32)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X32, df_32['log_units'])\n",
    "plt.scatter(X32[idx32], df_32.loc[idx32]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx32)), idx32, list(zip(X32[idx32], df_32.loc[idx32]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx32), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx32 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_32 :\n",
    "        if item_nbr == df_32.loc[num].item_nbr :\n",
    "            mean = df_32[df_32['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_32.set_value(num, 'log_units', mean)\n",
    "            df_32.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model32_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_32)\n",
    "result_model32_new = model32_new.fit()\n",
    "print(result_model32_new.summary())\n",
    "\n",
    "test_32_new = processed_test[processed_test['store_nbr'] == 32]\n",
    "\n",
    "test_32_new['log_units'] = result_model32_new.predict(test_32_new)\n",
    "test_32_new['units'] = np.exp(test_32_new['log_units']) - 1\n",
    "result_test2.append(test_32_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_33 = processed_train[processed_train['store_nbr'] == 33].reset_index(drop = True)\n",
    "\n",
    "model33 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_33)\n",
    "result_model33 = model33.fit()\n",
    "print(result_model33.summary())\n",
    "\n",
    "test_33 = processed_test[processed_test['store_nbr'] == 33]\n",
    "\n",
    "test_33['log_units'] = result_model33.predict(test_33)\n",
    "test_33['units'] = np.exp(test_33['log_units']) - 1\n",
    "result_test.append(test_33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv33_1 = df_33[:10101]\n",
    "cv33_2 = df_33[10101:20202]\n",
    "cv33_3 = df_33[20202:30414]\n",
    "cv33_4 = df_33[30414:40515]\n",
    "cv33_5 = df_33[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv33_train = pd.concat([cv33_1, cv33_2, cv33_3, cv33_4], ignore_index = True)\n",
    "cv33_test = cv33_5\n",
    "\n",
    "model_cv33 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv33_train)\n",
    "result_model_cv33 = model_cv33.fit()\n",
    "print(result_model_cv33.summary())\n",
    "\n",
    "cv33_test['log_units_cv'] = result_model_cv33.predict(cv33_test)\n",
    "cv33_test['units_cv'] = np.exp(cv33_test['log_units_cv']) - 1\n",
    "\n",
    "cv33_test['RMSLE'] = np.log((cv33_test['units_cv'] + 1) / (cv33_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv33_test['RMSLE']) / len(cv33_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_33 = get_item_nbr(df_33) #33번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence33 = result_model33.get_influence()\n",
    "\n",
    "cooks_d2_33, pvals33 = influence33.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr33 = 4 / (len(df_33) - 778)\n",
    "idx33 = np.where(cooks_d2_33 > fox_cr33)[0]\n",
    "\n",
    "print(len(idx33)) #outlier 갯수\n",
    "\n",
    "X33 = result_model33.predict(df_33)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X33, df_33['log_units'])\n",
    "plt.scatter(X33[idx33], df_33.loc[idx33]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx33)), idx33, list(zip(X33[idx33], df_33.loc[idx33]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx33), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx33 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_33 :\n",
    "        if item_nbr == df_33.loc[num].item_nbr :\n",
    "            mean = df_33[df_33['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_33.set_value(num, 'log_units', mean)\n",
    "            df_33.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model33_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_33)\n",
    "result_model33_new = model33_new.fit()\n",
    "print(result_model33_new.summary())\n",
    "\n",
    "test_33_new = processed_test[processed_test['store_nbr'] == 33]\n",
    "\n",
    "test_33_new['log_units'] = result_model33_new.predict(test_33_new)\n",
    "test_33_new['units'] = np.exp(test_33_new['log_units']) - 1\n",
    "result_test2.append(test_33_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_34 = processed_train[processed_train['store_nbr'] == 34].reset_index(drop = True)\n",
    "\n",
    "model34 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_34)\n",
    "result_model34 = model34.fit()\n",
    "print(result_model34.summary())\n",
    "\n",
    "test_34 = processed_test[processed_test['store_nbr'] == 34]\n",
    "\n",
    "test_34['log_units'] = result_model34.predict(test_34)\n",
    "test_34['units'] = np.exp(test_34['log_units']) - 1\n",
    "result_test.append(test_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv34_1 = df_34[:10101]\n",
    "cv34_2 = df_34[10101:20202]\n",
    "cv34_3 = df_34[20202:30414]\n",
    "cv34_4 = df_34[30414:40515]\n",
    "cv34_5 = df_34[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv34_train = pd.concat([cv34_1, cv34_2, cv34_3, cv34_4], ignore_index = True)\n",
    "cv34_test = cv34_5\n",
    "\n",
    "model_cv34 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv34_train)\n",
    "result_model_cv34 = model_cv34.fit()\n",
    "print(result_model_cv34.summary())\n",
    "\n",
    "cv34_test['log_units_cv'] = result_model_cv34.predict(cv34_test)\n",
    "cv34_test['units_cv'] = np.exp(cv34_test['log_units_cv']) - 1\n",
    "\n",
    "cv34_test['RMSLE'] = np.log((cv34_test['units_cv'] + 1) / (cv34_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv34_test['RMSLE']) / len(cv34_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_34 = get_item_nbr(df_34) #34번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence34 = result_model34.get_influence()\n",
    "\n",
    "cooks_d2_34, pvals34 = influence34.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr34 = 4 / (len(df_34) - 778)\n",
    "idx34 = np.where(cooks_d2_34 > fox_cr34)[0]\n",
    "\n",
    "print(len(idx34)) #outlier 갯수\n",
    "\n",
    "X34 = result_model34.predict(df_34)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X34, df_34['log_units'])\n",
    "plt.scatter(X34[idx34], df_34.loc[idx34]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx34)), idx34, list(zip(X34[idx34], df_34.loc[idx34]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx34), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx34 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_34 :\n",
    "        if item_nbr == df_34.loc[num].item_nbr :\n",
    "            mean = df_34[df_34['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_34.set_value(num, 'log_units', mean)\n",
    "            df_34.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model34_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_34)\n",
    "result_model34_new = model34_new.fit()\n",
    "print(result_model34_new.summary())\n",
    "\n",
    "test_34_new = processed_test[processed_test['store_nbr'] == 34]\n",
    "\n",
    "test_34_new['log_units'] = result_model34_new.predict(test_34_new)\n",
    "test_34_new['units'] = np.exp(test_34_new['log_units']) - 1\n",
    "result_test2.append(test_34_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_35 = processed_train[processed_train['store_nbr'] == 35].reset_index(drop = True)\n",
    "\n",
    "model35 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_35)\n",
    "result_model35 = model35.fit()\n",
    "print(result_model35.summary())\n",
    "\n",
    "test_35 = processed_test[processed_test['store_nbr'] == 35]\n",
    "\n",
    "test_35['log_units'] = result_model35.predict(test_35)\n",
    "test_35['units'] = np.exp(test_35['log_units']) - 1\n",
    "result_test.append(test_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv35_1 = df_35[:10101]\n",
    "cv35_2 = df_35[10101:20202]\n",
    "cv35_3 = df_35[20202:30414]\n",
    "cv35_4 = df_35[30414:40515]\n",
    "cv35_5 = df_35[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv35_train = pd.concat([cv35_1, cv35_2, cv35_3, cv35_4], ignore_index = True)\n",
    "cv35_test = cv35_5\n",
    "\n",
    "model_cv35 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv35_train)\n",
    "result_model_cv35 = model_cv35.fit()\n",
    "print(result_model_cv35.summary())\n",
    "\n",
    "cv35_test['log_units_cv'] = result_model_cv35.predict(cv35_test)\n",
    "cv35_test['units_cv'] = np.exp(cv35_test['log_units_cv']) - 1\n",
    "\n",
    "cv35_test['RMSLE'] = np.log((cv35_test['units_cv'] + 1) / (cv35_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv35_test['RMSLE']) / len(cv35_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_35 = get_item_nbr(df_35) #35번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence35 = result_model35.get_influence()\n",
    "\n",
    "cooks_d2_35, pvals35 = influence35.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr35 = 4 / (len(df_35) - 778)\n",
    "idx35 = np.where(cooks_d2_35 > fox_cr35)[0]\n",
    "\n",
    "print(len(idx35)) #outlier 갯수\n",
    "\n",
    "X35 = result_model35.predict(df_35)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X35, df_35['log_units'])\n",
    "plt.scatter(X35[idx35], df_35.loc[idx35]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx35)), idx35, list(zip(X35[idx35], df_35.loc[idx35]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx35), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx35 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_35 :\n",
    "        if item_nbr == df_35.loc[num].item_nbr :\n",
    "            mean = df_35[df_35['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_35.set_value(num, 'log_units', mean)\n",
    "            df_35.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model35_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_35)\n",
    "result_model35_new = model35_new.fit()\n",
    "print(result_model35_new.summary())\n",
    "\n",
    "test_35_new = processed_test[processed_test['store_nbr'] == 35]\n",
    "\n",
    "test_35_new['log_units'] = result_model35_new.predict(test_35_new)\n",
    "test_35_new['units'] = np.exp(test_35_new['log_units']) - 1\n",
    "result_test2.append(test_35_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_36 = processed_train[processed_train['store_nbr'] == 36].reset_index(drop = True)\n",
    "\n",
    "model36 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_36)\n",
    "result_model36 = model36.fit()\n",
    "print(result_model36.summary())\n",
    "\n",
    "test_36 = processed_test[processed_test['store_nbr'] == 36]\n",
    "\n",
    "test_36['log_units'] = result_model36.predict(test_36)\n",
    "test_36['units'] = np.exp(test_36['log_units']) - 1\n",
    "result_test.append(test_36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv36_1 = df_36[:10101]\n",
    "cv36_2 = df_36[10101:20202]\n",
    "cv36_3 = df_36[20202:30414]\n",
    "cv36_4 = df_36[30414:40515]\n",
    "cv36_5 = df_36[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv36_train = pd.concat([cv36_1, cv36_2, cv36_3, cv36_4], ignore_index = True)\n",
    "cv36_test = cv36_5\n",
    "\n",
    "model_cv36 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv36_train)\n",
    "result_model_cv36 = model_cv36.fit()\n",
    "print(result_model_cv36.summary())\n",
    "\n",
    "cv36_test['log_units_cv'] = result_model_cv36.predict(cv36_test)\n",
    "cv36_test['units_cv'] = np.exp(cv36_test['log_units_cv']) - 1\n",
    "\n",
    "cv36_test['RMSLE'] = np.log((cv36_test['units_cv'] + 1) / (cv36_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv36_test['RMSLE']) / len(cv36_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_36 = get_item_nbr(df_36) #36번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence36 = result_model36.get_influence()\n",
    "\n",
    "cooks_d2_36, pvals36 = influence36.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr36 = 4 / (len(df_36) - 778)\n",
    "idx36 = np.where(cooks_d2_36 > fox_cr36)[0]\n",
    "\n",
    "print(len(idx36)) #outlier 갯수\n",
    "\n",
    "X36 = result_model36.predict(df_36)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X36, df_36['log_units'])\n",
    "plt.scatter(X36[idx36], df_36.loc[idx36]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx36)), idx36, list(zip(X36[idx36], df_36.loc[idx36]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx36), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx36 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_36 :\n",
    "        if item_nbr == df_36.loc[num].item_nbr :\n",
    "            mean = df_36[df_36['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_36.set_value(num, 'log_units', mean)\n",
    "            df_36.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model36_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_36)\n",
    "result_model36_new = model36_new.fit()\n",
    "print(result_model36_new.summary())\n",
    "\n",
    "test_36_new = processed_test[processed_test['store_nbr'] == 36]\n",
    "\n",
    "test_36_new['log_units'] = result_model36_new.predict(test_36_new)\n",
    "test_36_new['units'] = np.exp(test_36_new['log_units']) - 1\n",
    "result_test2.append(test_36_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_37 = processed_train[processed_train['store_nbr'] == 37].reset_index(drop = True)\n",
    "\n",
    "model37 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_37)\n",
    "result_model37 = model37.fit()\n",
    "print(result_model37.summary())\n",
    "\n",
    "test_37 = processed_test[processed_test['store_nbr'] == 37]\n",
    "\n",
    "test_37['log_units'] = result_model37.predict(test_37)\n",
    "test_37['units'] = np.exp(test_37['log_units']) - 1\n",
    "result_test.append(test_37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv37_1 = df_37[:10101]\n",
    "cv37_2 = df_37[10101:20202]\n",
    "cv37_3 = df_37[20202:30414]\n",
    "cv37_4 = df_37[30414:40515]\n",
    "cv37_5 = df_37[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv37_train = pd.concat([cv37_1, cv37_2, cv37_3, cv37_4], ignore_index = True)\n",
    "cv37_test = cv37_5\n",
    "\n",
    "model_cv37 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv37_train)\n",
    "result_model_cv37 = model_cv37.fit()\n",
    "print(result_model_cv37.summary())\n",
    "\n",
    "cv37_test['log_units_cv'] = result_model_cv37.predict(cv37_test)\n",
    "cv37_test['units_cv'] = np.exp(cv37_test['log_units_cv']) - 1\n",
    "\n",
    "cv37_test['RMSLE'] = np.log((cv37_test['units_cv'] + 1) / (cv37_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv37_test['RMSLE']) / len(cv37_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_37 = get_item_nbr(df_37) #37번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence37 = result_model37.get_influence()\n",
    "\n",
    "cooks_d2_37, pvals37 = influence37.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr37 = 4 / (len(df_37) - 778)\n",
    "idx37 = np.where(cooks_d2_37 > fox_cr37)[0]\n",
    "\n",
    "print(len(idx37)) #outlier 갯수\n",
    "\n",
    "X37 = result_model37.predict(df_37)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X37, df_37['log_units'])\n",
    "plt.scatter(X37[idx37], df_37.loc[idx37]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx37)), idx37, list(zip(X37[idx37], df_37.loc[idx37]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx37), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx37 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_37 :\n",
    "        if item_nbr == df_37.loc[num].item_nbr :\n",
    "            mean = df_37[df_37['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_37.set_value(num, 'log_units', mean)\n",
    "            df_37.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model37_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_37)\n",
    "result_model37_new = model37_new.fit()\n",
    "print(result_model37_new.summary())\n",
    "\n",
    "test_37_new = processed_test[processed_test['store_nbr'] == 37]\n",
    "\n",
    "test_37_new['log_units'] = result_model37_new.predict(test_37_new)\n",
    "test_37_new['units'] = np.exp(test_37_new['log_units']) - 1\n",
    "result_test2.append(test_37_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_38 = processed_train[processed_train['store_nbr'] == 38].reset_index(drop = True)\n",
    "\n",
    "model38 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_38)\n",
    "result_model38 = model38.fit()\n",
    "print(result_model38.summary())\n",
    "\n",
    "test_38 = processed_test[processed_test['store_nbr'] == 38]\n",
    "\n",
    "test_38['log_units'] = result_model38.predict(test_38)\n",
    "test_38['units'] = np.exp(test_38['log_units']) - 1\n",
    "result_test.append(test_38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv38_1 = df_38[:10101]\n",
    "cv38_2 = df_38[10101:20202]\n",
    "cv38_3 = df_38[20202:30414]\n",
    "cv38_4 = df_38[30414:40515]\n",
    "cv38_5 = df_38[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv38_train = pd.concat([cv38_1, cv38_2, cv38_3, cv38_4], ignore_index = True)\n",
    "cv38_test = cv38_5\n",
    "\n",
    "model_cv38 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv38_train)\n",
    "result_model_cv38 = model_cv38.fit()\n",
    "print(result_model_cv38.summary())\n",
    "\n",
    "cv38_test['log_units_cv'] = result_model_cv38.predict(cv38_test)\n",
    "cv38_test['units_cv'] = np.exp(cv38_test['log_units_cv']) - 1\n",
    "\n",
    "cv38_test['RMSLE'] = np.log((cv38_test['units_cv'] + 1) / (cv38_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv38_test['RMSLE']) / len(cv38_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_38 = get_item_nbr(df_38) #38번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence38 = result_model38.get_influence()\n",
    "\n",
    "cooks_d2_38, pvals38 = influence38.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr38 = 4 / (len(df_38) - 778)\n",
    "idx38 = np.where(cooks_d2_38 > fox_cr38)[0]\n",
    "\n",
    "print(len(idx38)) #outlier 갯수\n",
    "\n",
    "X38 = result_model38.predict(df_38)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X38, df_38['log_units'])\n",
    "plt.scatter(X38[idx38], df_38.loc[idx38]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx38)), idx38, list(zip(X38[idx38], df_38.loc[idx38]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx38), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx38 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_38 :\n",
    "        if item_nbr == df_38.loc[num].item_nbr :\n",
    "            mean = df_38[df_38['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_38.set_value(num, 'log_units', mean)\n",
    "            df_38.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model38_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_38)\n",
    "result_model38_new = model38_new.fit()\n",
    "print(result_model38_new.summary())\n",
    "\n",
    "test_38_new = processed_test[processed_test['store_nbr'] == 38]\n",
    "\n",
    "test_38_new['log_units'] = result_model38_new.predict(test_38_new)\n",
    "test_38_new['units'] = np.exp(test_38_new['log_units']) - 1\n",
    "result_test2.append(test_38_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_39 = processed_train[processed_train['store_nbr'] == 39].reset_index(drop = True)\n",
    "\n",
    "model39 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_39)\n",
    "result_model39 = model39.fit()\n",
    "print(result_model39.summary())\n",
    "\n",
    "test_39 = processed_test[processed_test['store_nbr'] == 39]\n",
    "\n",
    "test_39['log_units'] = result_model39.predict(test_39)\n",
    "test_39['units'] = np.exp(test_39['log_units']) - 1\n",
    "result_test.append(test_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv39_1 = df_39[:10101]\n",
    "cv39_2 = df_39[10101:20202]\n",
    "cv39_3 = df_39[20202:30414]\n",
    "cv39_4 = df_39[30414:40515]\n",
    "cv39_5 = df_39[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv39_train = pd.concat([cv39_1, cv39_2, cv39_3, cv39_4], ignore_index = True)\n",
    "cv39_test = cv39_5\n",
    "\n",
    "model_cv39 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv39_train)\n",
    "result_model_cv39 = model_cv39.fit()\n",
    "print(result_model_cv39.summary())\n",
    "\n",
    "cv39_test['log_units_cv'] = result_model_cv39.predict(cv39_test)\n",
    "cv39_test['units_cv'] = np.exp(cv39_test['log_units_cv']) - 1\n",
    "\n",
    "cv39_test['RMSLE'] = np.log((cv39_test['units_cv'] + 1) / (cv39_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv39_test['RMSLE']) / len(cv39_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_39 = get_item_nbr(df_39) #39번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence39 = result_model39.get_influence()\n",
    "\n",
    "cooks_d2_39, pvals39 = influence39.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr39 = 4 / (len(df_39) - 778)\n",
    "idx39 = np.where(cooks_d2_39 > fox_cr39)[0]\n",
    "\n",
    "print(len(idx39)) #outlier 갯수\n",
    "\n",
    "X39 = result_model39.predict(df_39)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X39, df_39['log_units'])\n",
    "plt.scatter(X39[idx39], df_39.loc[idx39]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx39)), idx39, list(zip(X39[idx39], df_39.loc[idx39]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx39), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx39 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_39 :\n",
    "        if item_nbr == df_39.loc[num].item_nbr :\n",
    "            mean = df_39[df_39['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_39.set_value(num, 'log_units', mean)\n",
    "            df_39.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model39_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_39)\n",
    "result_model39_new = model39_new.fit()\n",
    "print(result_model39_new.summary())\n",
    "\n",
    "test_39_new = processed_test[processed_test['store_nbr'] == 39]\n",
    "\n",
    "test_39_new['log_units'] = result_model39_new.predict(test_39_new)\n",
    "test_39_new['units'] = np.exp(test_39_new['log_units']) - 1\n",
    "result_test2.append(test_39_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_40 = processed_train[processed_train['store_nbr'] == 40].reset_index(drop = True)\n",
    "\n",
    "model40 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_40)\n",
    "result_model40 = model40.fit()\n",
    "print(result_model40.summary())\n",
    "\n",
    "test_40 = processed_test[processed_test['store_nbr'] == 40]\n",
    "\n",
    "test_40['log_units'] = result_model40.predict(test_40)\n",
    "test_40['units'] = np.exp(test_40['log_units']) - 1\n",
    "result_test.append(test_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv40_1 = df_40[:10101]\n",
    "cv40_2 = df_40[10101:20202]\n",
    "cv40_3 = df_40[20202:30414]\n",
    "cv40_4 = df_40[30414:40515]\n",
    "cv40_5 = df_40[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv40_train = pd.concat([cv40_1, cv40_2, cv40_3, cv40_4], ignore_index = True)\n",
    "cv40_test = cv40_5\n",
    "\n",
    "model_cv40 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv40_train)\n",
    "result_model_cv40 = model_cv40.fit()\n",
    "print(result_model_cv40.summary())\n",
    "\n",
    "cv40_test['log_units_cv'] = result_model_cv40.predict(cv40_test)\n",
    "cv40_test['units_cv'] = np.exp(cv40_test['log_units_cv']) - 1\n",
    "\n",
    "cv40_test['RMSLE'] = np.log((cv40_test['units_cv'] + 1) / (cv40_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv40_test['RMSLE']) / len(cv40_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_40 = get_item_nbr(df_40) #40번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence40 = result_model40.get_influence()\n",
    "\n",
    "cooks_d2_40, pvals40 = influence40.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr40 = 4 / (len(df_40) - 778)\n",
    "idx40 = np.where(cooks_d2_40 > fox_cr40)[0]\n",
    "\n",
    "print(len(idx40)) #outlier 갯수\n",
    "\n",
    "X40 = result_model40.predict(df_40)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X40, df_40['log_units'])\n",
    "plt.scatter(X40[idx40], df_40.loc[idx40]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx40)), idx40, list(zip(X40[idx40], df_40.loc[idx40]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx40), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx40 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_40 :\n",
    "        if item_nbr == df_40.loc[num].item_nbr :\n",
    "            mean = df_40[df_40['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_40.set_value(num, 'log_units', mean)\n",
    "            df_40.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model40_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_40)\n",
    "result_model40_new = model40_new.fit()\n",
    "print(result_model40_new.summary())\n",
    "\n",
    "test_40_new = processed_test[processed_test['store_nbr'] == 40]\n",
    "\n",
    "test_40_new['log_units'] = result_model40_new.predict(test_40_new)\n",
    "test_40_new['units'] = np.exp(test_40_new['log_units']) - 1\n",
    "result_test2.append(test_40_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_41 = processed_train[processed_train['store_nbr'] == 41].reset_index(drop = True)\n",
    "\n",
    "model41 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_41)\n",
    "result_model41 = model41.fit()\n",
    "print(result_model41.summary())\n",
    "\n",
    "test_41 = processed_test[processed_test['store_nbr'] == 41]\n",
    "\n",
    "test_41['log_units'] = result_model41.predict(test_41)\n",
    "test_41['units'] = np.exp(test_41['log_units']) - 1\n",
    "result_test.append(test_41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv41_1 = df_41[:10101]\n",
    "cv41_2 = df_41[10101:20202]\n",
    "cv41_3 = df_41[20202:30414]\n",
    "cv41_4 = df_41[30414:40515]\n",
    "cv41_5 = df_41[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv41_train = pd.concat([cv41_1, cv41_2, cv41_3, cv41_4], ignore_index = True)\n",
    "cv41_test = cv41_5\n",
    "\n",
    "model_cv41 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv41_train)\n",
    "result_model_cv41 = model_cv41.fit()\n",
    "print(result_model_cv41.summary())\n",
    "\n",
    "cv41_test['log_units_cv'] = result_model_cv41.predict(cv41_test)\n",
    "cv41_test['units_cv'] = np.exp(cv41_test['log_units_cv']) - 1\n",
    "\n",
    "cv41_test['RMSLE'] = np.log((cv41_test['units_cv'] + 1) / (cv41_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv41_test['RMSLE']) / len(cv41_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_41 = get_item_nbr(df_41) #41번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence41 = result_model41.get_influence()\n",
    "\n",
    "cooks_d2_41, pvals41 = influence41.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr41 = 4 / (len(df_41) - 778)\n",
    "idx41 = np.where(cooks_d2_41 > fox_cr41)[0]\n",
    "\n",
    "print(len(idx41)) #outlier 갯수\n",
    "\n",
    "X41 = result_model41.predict(df_41)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X41, df_41['log_units'])\n",
    "plt.scatter(X41[idx41], df_41.loc[idx41]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx41)), idx41, list(zip(X41[idx41], df_41.loc[idx41]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx41), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx41 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_41 :\n",
    "        if item_nbr == df_41.loc[num].item_nbr :\n",
    "            mean = df_41[df_41['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_41.set_value(num, 'log_units', mean)\n",
    "            df_41.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model41_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_41)\n",
    "result_model41_new = model41_new.fit()\n",
    "print(result_model41_new.summary())\n",
    "\n",
    "test_41_new = processed_test[processed_test['store_nbr'] == 41]\n",
    "\n",
    "test_41_new['log_units'] = result_model41_new.predict(test_41_new)\n",
    "test_41_new['units'] = np.exp(test_41_new['log_units']) - 1\n",
    "result_test2.append(test_41_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_42 = processed_train[processed_train['store_nbr'] == 42].reset_index(drop = True)\n",
    "\n",
    "model42 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_42)\n",
    "result_model42 = model42.fit()\n",
    "print(result_model42.summary())\n",
    "\n",
    "test_42 = processed_test[processed_test['store_nbr'] == 42]\n",
    "\n",
    "test_42['log_units'] = result_model42.predict(test_42)\n",
    "test_42['units'] = np.exp(test_42['log_units']) - 1\n",
    "result_test.append(test_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv42_1 = df_42[:10101]\n",
    "cv42_2 = df_42[10101:20202]\n",
    "cv42_3 = df_42[20202:30414]\n",
    "cv42_4 = df_42[30414:40515]\n",
    "cv42_5 = df_42[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv42_train = pd.concat([cv42_1, cv42_2, cv42_3, cv42_4], ignore_index = True)\n",
    "cv42_test = cv42_5\n",
    "\n",
    "model_cv42 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv42_train)\n",
    "result_model_cv42 = model_cv42.fit()\n",
    "print(result_model_cv42.summary())\n",
    "\n",
    "cv42_test['log_units_cv'] = result_model_cv42.predict(cv42_test)\n",
    "cv42_test['units_cv'] = np.exp(cv42_test['log_units_cv']) - 1\n",
    "\n",
    "cv42_test['RMSLE'] = np.log((cv42_test['units_cv'] + 1) / (cv42_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv42_test['RMSLE']) / len(cv42_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_42 = get_item_nbr(df_42) #42번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence42 = result_model42.get_influence()\n",
    "\n",
    "cooks_d2_42, pvals42 = influence42.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr42 = 4 / (len(df_42) - 778)\n",
    "idx42 = np.where(cooks_d2_42 > fox_cr42)[0]\n",
    "\n",
    "print(len(idx42)) #outlier 갯수\n",
    "\n",
    "X42 = result_model42.predict(df_42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X42, df_42['log_units'])\n",
    "plt.scatter(X42[idx42], df_42.loc[idx42]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx42)), idx42, list(zip(X42[idx42], df_42.loc[idx42]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx42), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx42 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_42 :\n",
    "        if item_nbr == df_42.loc[num].item_nbr :\n",
    "            mean = df_42[df_42['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_42.set_value(num, 'log_units', mean)\n",
    "            df_42.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model42_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_42)\n",
    "result_model42_new = model42_new.fit()\n",
    "print(result_model42_new.summary())\n",
    "\n",
    "test_42_new = processed_test[processed_test['store_nbr'] == 42]\n",
    "\n",
    "test_42_new['log_units'] = result_model42_new.predict(test_42_new)\n",
    "test_42_new['units'] = np.exp(test_42_new['log_units']) - 1\n",
    "result_test2.append(test_42_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_43 = processed_train[processed_train['store_nbr'] == 43].reset_index(drop = True)\n",
    "\n",
    "model43 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_43)\n",
    "result_model43 = model43.fit()\n",
    "print(result_model43.summary())\n",
    "\n",
    "test_43 = processed_test[processed_test['store_nbr'] == 43]\n",
    "\n",
    "test_43['log_units'] = result_model43.predict(test_43)\n",
    "test_43['units'] = np.exp(test_43['log_units']) - 1\n",
    "result_test.append(test_43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv43_1 = df_43[:10101]\n",
    "cv43_2 = df_43[10101:20202]\n",
    "cv43_3 = df_43[20202:30414]\n",
    "cv43_4 = df_43[30414:40515]\n",
    "cv43_5 = df_43[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv43_train = pd.concat([cv43_1, cv43_2, cv43_3, cv43_4], ignore_index = True)\n",
    "cv43_test = cv43_5\n",
    "\n",
    "model_cv43 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv43_train)\n",
    "result_model_cv43 = model_cv43.fit()\n",
    "print(result_model_cv43.summary())\n",
    "\n",
    "cv43_test['log_units_cv'] = result_model_cv43.predict(cv43_test)\n",
    "cv43_test['units_cv'] = np.exp(cv43_test['log_units_cv']) - 1\n",
    "\n",
    "cv43_test['RMSLE'] = np.log((cv43_test['units_cv'] + 1) / (cv43_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv43_test['RMSLE']) / len(cv43_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_43 = get_item_nbr(df_43) #43번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence43 = result_model43.get_influence()\n",
    "\n",
    "cooks_d2_43, pvals43 = influence43.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr43 = 4 / (len(df_43) - 778)\n",
    "idx43 = np.where(cooks_d2_43 > fox_cr43)[0]\n",
    "\n",
    "print(len(idx43)) #outlier 갯수\n",
    "\n",
    "X43 = result_model43.predict(df_43)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X43, df_43['log_units'])\n",
    "plt.scatter(X43[idx43], df_43.loc[idx43]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx43)), idx43, list(zip(X43[idx43], df_43.loc[idx43]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx43), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx43 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_43 :\n",
    "        if item_nbr == df_43.loc[num].item_nbr :\n",
    "            mean = df_43[df_43['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_43.set_value(num, 'log_units', mean)\n",
    "            df_43.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model43_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_43)\n",
    "result_model43_new = model43_new.fit()\n",
    "print(result_model43_new.summary())\n",
    "\n",
    "test_43_new = processed_test[processed_test['store_nbr'] == 43]\n",
    "\n",
    "test_43_new['log_units'] = result_model43_new.predict(test_43_new)\n",
    "test_43_new['units'] = np.exp(test_43_new['log_units']) - 1\n",
    "result_test2.append(test_43_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_44 = processed_train[processed_train['store_nbr'] == 44].reset_index(drop = True)\n",
    "\n",
    "model44 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_44)\n",
    "result_model44 = model44.fit()\n",
    "print(result_model44.summary())\n",
    "\n",
    "test_44 = processed_test[processed_test['store_nbr'] == 44]\n",
    "\n",
    "test_44['log_units'] = result_model44.predict(test_44)\n",
    "test_44['units'] = np.exp(test_44['log_units']) - 1\n",
    "result_test.append(test_44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv44_1 = df_44[:10101]\n",
    "cv44_2 = df_44[10101:20202]\n",
    "cv44_3 = df_44[20202:30414]\n",
    "cv44_4 = df_44[30414:40515]\n",
    "cv44_5 = df_44[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv44_train = pd.concat([cv44_1, cv44_2, cv44_3, cv44_4], ignore_index = True)\n",
    "cv44_test = cv44_5\n",
    "\n",
    "model_cv44 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv44_train)\n",
    "result_model_cv44 = model_cv44.fit()\n",
    "print(result_model_cv44.summary())\n",
    "\n",
    "cv44_test['log_units_cv'] = result_model_cv44.predict(cv44_test)\n",
    "cv44_test['units_cv'] = np.exp(cv44_test['log_units_cv']) - 1\n",
    "\n",
    "cv44_test['RMSLE'] = np.log((cv44_test['units_cv'] + 1) / (cv44_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv44_test['RMSLE']) / len(cv44_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_44 = get_item_nbr(df_44) #44번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence44 = result_model44.get_influence()\n",
    "\n",
    "cooks_d2_44, pvals44 = influence44.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr44 = 4 / (len(df_44) - 778)\n",
    "idx44 = np.where(cooks_d2_44 > fox_cr44)[0]\n",
    "\n",
    "print(len(idx44)) #outlier 갯수\n",
    "\n",
    "X44 = result_model44.predict(df_44)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X44, df_44['log_units'])\n",
    "plt.scatter(X44[idx44], df_44.loc[idx44]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx44)), idx44, list(zip(X44[idx44], df_44.loc[idx44]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx44), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx44 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_44 :\n",
    "        if item_nbr == df_44.loc[num].item_nbr :\n",
    "            mean = df_44[df_44['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_44.set_value(num, 'log_units', mean)\n",
    "            df_44.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model44_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_44)\n",
    "result_model44_new = model44_new.fit()\n",
    "print(result_model44_new.summary())\n",
    "\n",
    "test_44_new = processed_test[processed_test['store_nbr'] == 44]\n",
    "\n",
    "test_44_new['log_units'] = result_model44_new.predict(test_44_new)\n",
    "test_44_new['units'] = np.exp(test_44_new['log_units']) - 1\n",
    "result_test2.append(test_44_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_45 = processed_train[processed_train['store_nbr'] == 45].reset_index(drop = True)\n",
    "\n",
    "model45 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_45)\n",
    "result_model45 = model45.fit()\n",
    "print(result_model45.summary())\n",
    "\n",
    "test_45 = processed_test[processed_test['store_nbr'] == 45]\n",
    "\n",
    "test_45['log_units'] = result_model45.predict(test_45)\n",
    "test_45['units'] = np.exp(test_45['log_units']) - 1\n",
    "result_test.append(test_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# train set이 총 5분기 이므로 1분기씩 나눠준다..\n",
    "cv45_1 = df_45[:10101]\n",
    "cv45_2 = df_45[10101:20202]\n",
    "cv45_3 = df_45[20202:30414]\n",
    "cv45_4 = df_45[30414:40515]\n",
    "cv45_5 = df_45[40515:]\n",
    "\n",
    "# 기존 train set을 cv_train과 cv_test로 나눌 수 있음\n",
    "cv45_train = pd.concat([cv45_1, cv45_2, cv45_3, cv45_4], ignore_index = True)\n",
    "cv45_test = cv45_5\n",
    "\n",
    "model_cv45 = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = cv45_train)\n",
    "result_model_cv45 = model_cv45.fit()\n",
    "print(result_model_cv45.summary())\n",
    "\n",
    "cv45_test['log_units_cv'] = result_model_cv45.predict(cv45_test)\n",
    "cv45_test['units_cv'] = np.exp(cv45_test['log_units_cv']) - 1\n",
    "\n",
    "cv45_test['RMSLE'] = np.log((cv45_test['units_cv'] + 1) / (cv45_test['units'] + 1)) ** 2\n",
    "RMSLE = np.sqrt(np.sum(cv45_test['RMSLE']) / len(cv45_test))\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_nbr_list_45 = get_item_nbr(df_45) #45번 store에서 팔린 item_nbr만 list로 저장\n",
    "\n",
    "influence45 = result_model45.get_influence()\n",
    "\n",
    "cooks_d2_45, pvals45 = influence45.cooks_distance # fox outlier recommendation으로 outlier 판별\n",
    "fox_cr45 = 4 / (len(df_45) - 778)\n",
    "idx45 = np.where(cooks_d2_45 > fox_cr45)[0]\n",
    "\n",
    "print(len(idx45)) #outlier 갯수\n",
    "\n",
    "X45 = result_model45.predict(df_45)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.scatter(X45, df_45['log_units'])\n",
    "plt.scatter(X45[idx45], df_45.loc[idx45]['log_units'], s = 100, c = \"r\", alpha = 0.5)\n",
    "utils.annotate_axes(range(len(idx45)), idx45, list(zip(X45[idx45], df_45.loc[idx45]['log_units'])),\n",
    "                    [(-20, 15)] * len(idx45), size = \"small\", ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in idx45 : # outlier들을 각 item_nbr별 평균치로 대체\n",
    "    for item_nbr in item_nbr_list_45 :\n",
    "        if item_nbr == df_45.loc[num].item_nbr :\n",
    "            mean = df_45[df_45['item_nbr'] == item_nbr]['log_units'].mean()\n",
    "            df_45.set_value(num, 'log_units', mean)\n",
    "            df_45.set_value(num, 'units', np.exp(mean) - 1)\n",
    "\n",
    "model45_new = sm.OLS.from_formula('log_units ~ C(item_nbr):C(weekday, contrast_weekday) + 0', data = df_45)\n",
    "result_model45_new = model45_new.fit()\n",
    "print(result_model45_new.summary())\n",
    "\n",
    "test_45_new = processed_test[processed_test['store_nbr'] == 45]\n",
    "\n",
    "test_45_new['log_units'] = result_model45_new.predict(test_45_new)\n",
    "test_45_new['units'] = np.exp(test_45_new['log_units']) - 1\n",
    "result_test2.append(test_45_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.concat(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(sub), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.drop(['weekday', 'holiday', 'event'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.sort_values(by = ['date', 'store_nbr', 'item_nbr'], inplace = True)\n",
    "sub.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units = sub['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['store_nbr'] = sub['store_nbr'].astype('str')\n",
    "sub['item_nbr'] = sub['item_nbr'].astype('str')\n",
    "sub['date'] = sub['date'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['id'] = sub['store_nbr'] + '_' + sub['item_nbr'] + '_' + sub['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.drop(['date', 'store_nbr', 'item_nbr', 'log_units', 'units'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['units'] = units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('sub.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = '../sub_test11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = '../sub_test11_rank.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier 대체 후"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = '../outlier.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
